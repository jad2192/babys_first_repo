{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import log, exp, zeros, dot, array\n",
    "import itertools\n",
    "from itertools import product\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(x):\n",
    "    \n",
    "    m = x.max()\n",
    "    x_s = x - m\n",
    "    return m + log((exp(x_s)).sum())\n",
    "\n",
    "\n",
    "def label_seq(l,j):\n",
    "    '''Creates a list of all posible length l sequences taking values in\n",
    "       {0,1,...,j-1}. Out put list is of length j^l'''\n",
    "    \n",
    "    \n",
    "    return list(product(list(range(j)), repeat=l))\n",
    "\n",
    "def find_legit_vals(k,l,n):\n",
    "    '''Finds the only possible label pairs (i,j) with f_k(i,j,x,t) != 0.\n",
    "       Will be used to speed up gradient computation.\n",
    "       l: Number of labels\n",
    "       n: Size of a nodes feature vector'''\n",
    "    \n",
    "    res = []\n",
    "    k_s = k - n*l\n",
    "    \n",
    "    if k_s < 0:  #Means k is in the Unary features\n",
    "        \n",
    "        i = k // n\n",
    "        \n",
    "        for j in range(l):\n",
    "            \n",
    "            res.append((i,j))\n",
    "            \n",
    "    else:  #Means k is a Binary feature\n",
    "        \n",
    "        f = (k_s // n)\n",
    "        j = f % l\n",
    "        i = f // l\n",
    "        res.append((i,j))  \n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "class CRF(object):\n",
    "    \n",
    "    def __init__(self, feature_function, K, T, L, lamb, W='n'):\n",
    "        ''' If our labels belong to space L and our observations belonging to X amd\n",
    "            then length of our chain is T (call {0,1, ... , T-1} = T') then \n",
    "            feature_functions is a vector valued function, f: L^2 x X x T' --> R^K\n",
    "            i.e. f(i,j,x,t) is a K-d real valued vector and has component functions of the form\n",
    "            f_k(i,j,x,t) to be specified, think of (i,j) = (y_t, y_t-1). It will have an optional\n",
    "            keyword argument 'project' which defaults to -1, if a positive integer k,\n",
    "            it will project onto the k-th component.\n",
    "            \n",
    "            K: # of Features\n",
    "            \n",
    "            T: Length of Chain\n",
    "            \n",
    "            L: Number of labels. Will assume labels have been encoded as integers in {0,...,L-1} \n",
    "            \n",
    "            lamb: L2 regularization constant'''\n",
    "        \n",
    "        self.f_x = feature_function\n",
    "        if type(W) == str:\n",
    "            self.W = np.random.uniform(low=-0.5, high=0.5, size=K)\n",
    "        else:\n",
    "            self.W = W\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.L = L\n",
    "        self.Lambda = lamb\n",
    "        \n",
    "    def  log_forward(self, x):\n",
    "        '''This computes the log(alphas) as in the forward-backward algorithm in order to\n",
    "           be used for inference tasks later on.\n",
    "           x is an observation.'''\n",
    "        \n",
    "        f = self.f_x\n",
    "        alphas = zeros((self.T, self.L))\n",
    "        \n",
    "        # Initialization\n",
    "        \n",
    "        for l in range(self.L):\n",
    "            \n",
    "            alphas[0,l] = dot(self.W, f(l,0,x,0))\n",
    "            \n",
    "        # Recursion\n",
    "        \n",
    "        for t in range(1,self.T):\n",
    "            \n",
    "            for l in range(self.L):\n",
    "                \n",
    "                psi = array([dot(self.W, f(l,i,x,t)) for i in range(self.L)])\n",
    "                \n",
    "                alphas[t,l] = log_sum_exp(psi + alphas[t-1])\n",
    "            \n",
    "        return alphas\n",
    "    \n",
    "        \n",
    "    def log_backward(self, x):\n",
    "        '''This computes the log(betas) as in the forward-backward algorithm in order to\n",
    "           be used for inference tasks later on.\n",
    "           x is an observation.'''\n",
    "        \n",
    "        # Initialization\n",
    "        \n",
    "        f = self.f_x\n",
    "        betas = np.ones((self.T, self.L))\n",
    "        \n",
    "        # Recursion\n",
    "        \n",
    "        for t in range(self.T-2,-1,-1):\n",
    "            \n",
    "            for l in range(self.L):\n",
    "                \n",
    "                psi = array([dot(self.W, f(i,l,x,t+1)) for i in range(self.L)])\n",
    "                \n",
    "                betas[t][l] = log_sum_exp(psi + betas[t+1])\n",
    "                \n",
    "        return betas\n",
    "    \n",
    "    \n",
    "    def log_partition(self, x):\n",
    "        '''Efficient computation of the log of the partition function Z(x) appearing in CRF model.\n",
    "           Input an observation and inital label (for forward algorithm) and output is log(Z(x))'''\n",
    "        \n",
    "        alphas = self.log_forward(x)\n",
    "        \n",
    "        return log_sum_exp(alphas[-1])\n",
    "    \n",
    "    \n",
    "    def MAP(self, x):\n",
    "        '''Viterbi algortithm for computing the most likely label of a sequence with\n",
    "           given observation vector x using maximum a posteriori estimation. Using log\n",
    "           sum version for numeric stability'''\n",
    "        \n",
    "        f = self.f_x\n",
    "            \n",
    "        # Initialization\n",
    "        \n",
    "        deltas = np.zeros((self.T, self.L))\n",
    "        delt_arg = np.zeros((self.T, self.L))\n",
    "        \n",
    "        for l in range(self.L):\n",
    "            \n",
    "            deltas[0][l] = dot(self.W, f(l,0,x,0))  # Not sure about this.\n",
    "            \n",
    "        # Recursion\n",
    "        \n",
    "        for t in range(1,self.T):\n",
    "            \n",
    "            for l in range(self.L):\n",
    "                \n",
    "                psi = array([dot(self.W, f(l,i,x,t)) for i in range(self.L)])\n",
    "                \n",
    "                deltas[t][l] = (psi + deltas[t-1]).max()\n",
    "                delt_arg[t][l] = (psi + deltas[t-1]).argmax()\n",
    "            \n",
    "        map_lab = np.zeros(self.T, dtype='i4')\n",
    "        map_lab[-1] = deltas[-1].argmax()\n",
    "        \n",
    "        for t in range(self.T-2,-1,-1):\n",
    "            \n",
    "            map_lab[t] = delt_arg[t+1][map_lab[t+1]]\n",
    "            \n",
    "                \n",
    "        return tuple(map_lab)\n",
    "        \n",
    "        \n",
    "    def marginal(self,i,j,x,t):\n",
    "        '''Using the forward backward algorithm to compute the marginal p(y_t-1=i,y_t=j|x)'''\n",
    "        \n",
    "        f = self.f_x\n",
    "        alphas = self.log_forward(x)\n",
    "        betas = self.log_backward(x)\n",
    "        psi = dot(self.W,f(j,i,x,t))\n",
    "        psi_b = np.array([dot(self.W,f(k,0,x,0)) for k in range(self.L)])\n",
    "        log_joint = alphas[t-1][i] + psi + betas[t][j] - log_sum_exp(psi_b + betas[0])\n",
    "        \n",
    "        return exp(log_joint)\n",
    "                       \n",
    "    def naive_comp(self, x, out='Z'):\n",
    "        '''Brute force computation of log(Z(x)) or MAP (if out = 'MAP')'''\n",
    "        \n",
    "        f = self.f_x\n",
    "        \n",
    "        # Get List of all possible label sequences\n",
    "        \n",
    "        lab_seq = label_seq(self.T, self.L)\n",
    "        \n",
    "        psi = np.zeros(len(lab_seq))\n",
    "        \n",
    "        for k in range(len(lab_seq)):\n",
    "            \n",
    "            lab = lab_seq[k]\n",
    "            temp = np.zeros(self.T)\n",
    "            temp[0] = dot(self.W, f(lab[0], 0, x, 0))\n",
    "            \n",
    "            for t in range(1,self.T):\n",
    "                \n",
    "                temp[t] = dot(self.W, f(lab[t], lab[t-1], x, t))\n",
    "            \n",
    "            psi[k] = temp.sum()\n",
    "        \n",
    "        arg_m_i = psi.argmax()\n",
    "        \n",
    "        return log_sum_exp(psi) if out == 'Z' else lab_seq[arg_m_i]\n",
    "        \n",
    "        \n",
    "    \n",
    "    def gradient(self, X, Y):\n",
    "        ''' Creates the gradient vector of the log-likelihood. \n",
    "            X, Y: Are arrays containing training examples.'''\n",
    "        \n",
    "        f = self.f_x\n",
    "        lamb = self.Lambda\n",
    "        grad = np.zeros(self.K)\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            \n",
    "            val_pair = find_legit_vals(k, self.L, X.shape[-1])\n",
    "            first_term = np.zeros((X.shape[0],self.T))\n",
    "            \n",
    "            for n in range(X.shape[0]):\n",
    "                \n",
    "                for t in range(self.T):\n",
    "                    \n",
    "                    first_term[n][t] = f(Y[n][t], Y[n][t-1], X[n], t, project=k)\n",
    "            \n",
    "            sec_term = np.zeros((X.shape[0],self.T))\n",
    "            \n",
    "            for n in range(X.shape[0]):\n",
    "                \n",
    "                for t in range(self.T):\n",
    "                    \n",
    "                    marginals = np.zeros(len(val_pair))\n",
    "                        \n",
    "                    for j in range(len(val_pair)):\n",
    "                        \n",
    "                        y, y_p = val_pair[j]\n",
    "                        marginals[j] = (f(y, y_p, X[n], t, project=k) * \n",
    "                                        self.marginal(y, y_p, X[n], t))\n",
    "                    \n",
    "                    sec_term[n][t] = marginals.sum()\n",
    "                    \n",
    "            grad[k] = (first_term + sec_term).sum() - self.W[k] * lamb\n",
    "            print(k)\n",
    "        return grad\n",
    "    \n",
    "    \n",
    "    def gradient_f(self, X, Y):\n",
    "        \n",
    "        f = self.f_x\n",
    "        lamb = self.Lambda\n",
    "        grad_f = np.zeros(self.K)\n",
    "        lab_pairs = label_seq(2,self.L)\n",
    "        \n",
    "        for n in range(X.shape[0]):\n",
    "            \n",
    "            grad = np.zeros(self.K)\n",
    "            x, y = X[n], Y[n]\n",
    "            \n",
    "            for t in range(self.T):\n",
    "            \n",
    "                grad = grad + f(y[t],y[t-1],x,t)\n",
    "        \n",
    "            for pair in lab_pairs:\n",
    "            \n",
    "                y, y_p = pair[0], pair[1]\n",
    "                grad_temp = np.zeros(self.K)\n",
    "            \n",
    "                for t in range(self.T):\n",
    "                \n",
    "                    marg = self.marginal(y,y_p,x,t)\n",
    "                    grad_temp = grad_temp + (marg*f(y,y_p,x,t))\n",
    "        \n",
    "                grad = grad - grad_temp\n",
    "            \n",
    "            grad_f = grad_f + grad\n",
    "    \n",
    "        return lamb * self.W  - grad_f\n",
    "    \n",
    "    def reg_neg_ll(self, X, Y):\n",
    "        \n",
    "        f = self.f_x\n",
    "        lamb = self.Lambda\n",
    "        res = 0\n",
    "        for n in range(X.shape[0]):\n",
    "            \n",
    "            s = 0\n",
    "            \n",
    "            for t in range(self.T):\n",
    "            \n",
    "                s += np.dot(self.W,f(Y[n,t],Y[n,t-1],X[n],t))\n",
    "            \n",
    "            res += self.log_partition(X[n]) - s\n",
    "         \n",
    "        return res + (0.5 * lamb * np.dot(self.W, self.W))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feat_map(i, j, x, t, projection=-1):\n",
    "    \n",
    "    unary_chi = np.zeros(15)\n",
    "    binary_chi = np.zeros(45)\n",
    "    \n",
    "    unary_chi[5*i:5*(i+1)] = (t >=0) * x[t]\n",
    "    \n",
    "    if t > 0:\n",
    "        \n",
    "        binary_chi[5*(3*i+j): 5*(3*i+j+1)] = x[t] + x[(t-1)]\n",
    "        \n",
    "    if t == 0 and j == 0:\n",
    "        \n",
    "        binary_chi[5*(3*i+j): 5*(3*i+j+1)] = x[0]\n",
    "    \n",
    "    output = np.zeros(3*5 + 9*5)\n",
    "    output[:3*5] = unary_chi\n",
    "    output[3*5:] = binary_chi\n",
    "    \n",
    "    if projection > 0:\n",
    "        \n",
    "        return output[projection]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return binary_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate simple test data: Length 5 chains, 3 class labels, each feature vector is 5 dimensional\n",
    "\n",
    "features = np.zeros((99, 5, 5))\n",
    "\n",
    "for k in range(33):\n",
    "    \n",
    "    for j in range(5):\n",
    "        \n",
    "        features[k,j,0] = np.random.randn()\n",
    "        features[k+33,j,2] = np.random.randn()\n",
    "        features[k+66,j,4] = np.random.randn()\n",
    "\n",
    "labels = np.zeros(99)\n",
    "\n",
    "for k in range(33):\n",
    "    \n",
    "    labels[k+33] = 1\n",
    "    labels[k+66] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31992262,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.05266409,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.09258435,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 2.1222034 ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.43536307,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crf_test = CRF(feat_map, 45, 5, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 1, 2, 2)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_test.MAP(features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 1, 2, 2)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_test.naive_comp(features[1],'MAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.5213871355553366"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_test.naive_comp(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.5213871355553374"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_test.log_partition(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_test = np.array([crf_test.marginal(i,j,features[12],2) for i in range(3) for j in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999999999978"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.:/home/james/anaconda3/lib/python3.5/site-packages/oct2py:/home/james/anaconda3/lib/python3.5/site-packages/octave_kernel:/usr/lib/x86_64-linux-gnu/octave/4.0.3/site/oct/x86_64-pc-linux-gnu:/usr/lib/x86_64-linux-gnu/octave/site/oct/api-v50+/x86_64-pc-linux-gnu:/usr/lib/x86_64-linux-gnu/octave/site/oct/x86_64-pc-linux-gnu:/usr/share/octave/4.0.3/site/m:/usr/share/octave/site/api-v50+/m:/usr/share/octave/site/m:/usr/share/octave/site/m/startup:/usr/lib/x86_64-linux-gnu/octave/4.0.3/oct/x86_64-pc-linux-gnu:/usr/share/octave/4.0.3/m:/usr/share/octave/4.0.3/m/audio:/usr/share/octave/4.0.3/m/debian:/usr/share/octave/4.0.3/m/deprecated:/usr/share/octave/4.0.3/m/elfun:/usr/share/octave/4.0.3/m/general:/usr/share/octave/4.0.3/m/geometry:/usr/share/octave/4.0.3/m/gui:/usr/share/octave/4.0.3/m/help:/usr/share/octave/4.0.3/m/image:/usr/share/octave/4.0.3/m/io:/usr/share/octave/4.0.3/m/java:/usr/share/octave/4.0.3/m/linear-algebra:/usr/share/octave/4.0.3/m/miscellaneous:/usr/share/octave/4.0.3/m/optimization:/usr/share/octave/4.0.3/m/path:/usr/share/octave/4.0.3/m/pkg:/usr/share/octave/4.0.3/m/plot:/usr/share/octave/4.0.3/m/plot/appearance:/usr/share/octave/4.0.3/m/plot/draw:/usr/share/octave/4.0.3/m/plot/util:/usr/share/octave/4.0.3/m/polynomial:/usr/share/octave/4.0.3/m/prefs:/usr/share/octave/4.0.3/m/set:/usr/share/octave/4.0.3/m/signal:/usr/share/octave/4.0.3/m/sparse:/usr/share/octave/4.0.3/m/specfun:/usr/share/octave/4.0.3/m/special-matrix:/usr/share/octave/4.0.3/m/startup:/usr/share/octave/4.0.3/m/statistics:/usr/share/octave/4.0.3/m/statistics/base:/usr/share/octave/4.0.3/m/statistics/distributions:/usr/share/octave/4.0.3/m/statistics/models:/usr/share/octave/4.0.3/m/statistics/tests:/usr/share/octave/4.0.3/m/strings:/usr/share/octave/4.0.3/m/testfun:/usr/share/octave/4.0.3/m/time:/usr/share/octave/4.0.3/data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from oct2py import octave\n",
    "octave.addpath('/home/james/anaconda3/data/HW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = octave.data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feat_map_toy(i, j, x, t, project=-1):\n",
    "    ''' Change the values of feature vector length and number of classes. '''\n",
    "    \n",
    "    N = 3                            # length of data vector\n",
    "    L = 3                          # number of labels\n",
    "    T = 10                          # length of sequence\n",
    "    unary_chi = np.zeros(N * L)\n",
    "    binary_chi = np.zeros(N * L**2)\n",
    "    \n",
    "    unary_chi[N*i:N*(i+1)] = x[t]\n",
    "    \n",
    "    if t > 0:\n",
    "        \n",
    "        binary_chi[N*(L*i+j): N*(L*i+j+1)] = x[t] + x[(t-1)]\n",
    "        \n",
    "    if t == 0 and j == 0:\n",
    "        \n",
    "        binary_chi[N*(L*i+j): N*(L*i+j+1)] = x[t]\n",
    "    \n",
    "    \n",
    "    output = np.zeros(N*L + N*L**2)\n",
    "    output[:N*L] = unary_chi\n",
    "    output[N*L:] = binary_chi\n",
    "    \n",
    "    if project > -1:\n",
    "        \n",
    "        return output[project]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_f = np.zeros((1000,10,3))\n",
    "\n",
    "for k in range(1000):\n",
    "    \n",
    "    for j in range(10):\n",
    "        \n",
    "        X_f[k,j,:] = X[k][0][:,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_f = np.zeros((1000,10))\n",
    "\n",
    "for k in range(1000):\n",
    "    \n",
    "    y_f[k] = X[k,1][0] -1\n",
    "    \n",
    "y_f = np.asarray(y_f,dtype='i4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, ..., 2, 2, 2],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [2, 2, 2, ..., 2, 2, 2],\n",
       "       ..., \n",
       "       [2, 2, 2, ..., 2, 2, 2],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [2, 2, 2, ..., 2, 2, 2]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and Test Split\n",
    "\n",
    "dims = np.arange(X_f.shape[0])\n",
    "np.random.shuffle(dims)\n",
    "\n",
    "X_tr, y_tr = X_f[dims[:-100]], y_f[dims[:-100]]\n",
    "X_ts, y_ts = X_f[dims[-100:]], y_f[dims[-100:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch/Iteration:  1 / 0 . Current Average Test Hamming Accuracy:  35.6666666667 %.\n",
      "Epoch/Iteration:  1 / 20 . Current Average Test Hamming Accuracy:  40.0 %.\n",
      "Epoch/Iteration:  1 / 40 . Current Average Test Hamming Accuracy:  59.0 %.\n",
      "Epoch/Iteration:  1 / 60 . Current Average Test Hamming Accuracy:  74.0 %.\n",
      "Epoch/Iteration:  1 / 80 . Current Average Test Hamming Accuracy:  77.3333333333 %.\n",
      "Epoch/Iteration:  1 / 100 . Current Average Test Hamming Accuracy:  79.0 %.\n",
      "Epoch/Iteration:  1 / 120 . Current Average Test Hamming Accuracy:  82.3333333333 %.\n",
      "Epoch/Iteration:  1 / 140 . Current Average Test Hamming Accuracy:  86.6666666667 %.\n",
      "Epoch/Iteration:  1 / 160 . Current Average Test Hamming Accuracy:  93.0 %.\n",
      "Epoch/Iteration:  1 / 180 . Current Average Test Hamming Accuracy:  94.6666666667 %.\n",
      "Epoch/Iteration:  1 / 200 . Current Average Test Hamming Accuracy:  96.0 %.\n",
      "Epoch/Iteration:  2 / 56 . Current Average Test Hamming Accuracy:  96.0 %.\n",
      "Epoch/Iteration:  3 / 112 . Current Average Test Hamming Accuracy:  96.0 %.\n"
     ]
    }
   ],
   "source": [
    "# Adam stochastic mini-batch gradient descent on the (negative) log liklelihood.\n",
    "from scipy.spatial.distance import hamming \n",
    "\n",
    "crf_test1 = CRF(feat_map_toy, 3**2+ 3**3,10, 3, 0)\n",
    "alpha = 0.03\n",
    "B1, B2 = 0.9, 0.999\n",
    "eps = 1e-7\n",
    "m, v = np.zeros(crf_test1.K), np.zeros(crf_test1.K)\n",
    "num_epochs = 3\n",
    "batch_size = 16\n",
    "epoch = 1\n",
    "\n",
    "while epoch <= num_epochs:\n",
    "    \n",
    "    min_batch = np.arange(X_tr.shape[0])\n",
    "    np.random.shuffle(min_batch)\n",
    "    \n",
    "    for k in range(0, X_tr.shape[0],batch_size):\n",
    "        \n",
    "        X_b, y_b = X_tr[min_batch[k:k+batch_size]],y_tr[min_batch[k:k+batch_size]]\n",
    "        t = (X_tr.shape[0]//batch_size)*(epoch-1) + (k // 4) + 1\n",
    "        g = crf_test1.gradient_f(X_b, y_b)\n",
    "        m = B1 * m + (1-B1) * g\n",
    "        v = B2 * v + np.square(np.sqrt(1-B2) * g)\n",
    "        m_b = m / (1 - B1**t)\n",
    "        v_b = v / (1 - B2**t)\n",
    "        crf_test1.W = crf_test1.W - (alpha * m_b) / (np.sqrt(v_b) + eps)\n",
    "        \n",
    "        if (k // batch_size) % 5 == 0:\n",
    "            \n",
    "            acc_test = np.zeros(30)\n",
    "            \n",
    "            for n in range(30):\n",
    "                acc_test[n] = 1 - hamming(crf_test1.MAP(X_ts[n]), y_ts[n])\n",
    "            print('Epoch/Iteration: ', epoch, '/', t-1, '. Current Average Test Hamming Accuracy: ',\n",
    "                  100* acc_test.mean(), '%.')\n",
    "        \n",
    "        if acc_test.mean() >= 0.96:\n",
    "            break\n",
    "            epoch = num_epochs+1\n",
    "        \n",
    "          \n",
    "    epoch += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_test1.MAP(X_f[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_f[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.954\n",
      "0.62\n"
     ]
    }
   ],
   "source": [
    "results = np.zeros(100)\n",
    "for k in range(100):\n",
    "    results[k] = 1 - hamming(crf_test1.MAP(X_ts[k]), y_ts[k])\n",
    "print(results.mean())\n",
    "print(results[results==1].sum() / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node = 0\n",
    "for k in range(y_tr.shape[0]):\n",
    "    for j in range(y_tr[k].shape[0]):\n",
    "        num_node += 1\n",
    "        \n",
    "\n",
    "X_s, y_s = np.zeros((num_node,3)), np.zeros(num_node,dtype='i4')\n",
    "l = 0\n",
    "for k in range(y_tr.shape[0]):\n",
    "    for j in range(y_tr[k].shape[0]):\n",
    "        X_s[l] = X_tr[k][j,:]\n",
    "        y_s[l] = y_tr[k][j]\n",
    "        l+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_s = SVC(kernel='linear')\n",
    "clf_s.fit(X_s,y_s)\n",
    "clf_l = LogisticRegression()\n",
    "clf_l.fit(X_s,y_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Avg Hamming:  92.3 %\n",
      "SVC Zero-One loss:  48.0\n",
      "Log Avg Hamming:  92.3 %\n",
      "Log Zero-One loss:  49.0\n"
     ]
    }
   ],
   "source": [
    "ham_svc = np.zeros(y_ts.shape[0])\n",
    "ham_l = np.zeros(y_ts.shape[0])\n",
    "for k in range(y_ts.shape[0]):\n",
    "    ham_svc[k] = accuracy_score(y_ts[k], clf_s.predict(X_ts[k]))\n",
    "    ham_l[k] = accuracy_score(y_ts[k], clf_l.predict(X_ts[k]))\n",
    "    \n",
    "print('SVC Avg Hamming: ', 100*ham_svc.mean(), '%')\n",
    "print('SVC Zero-One loss: ', 100*ham_svc[ham_svc==1].sum()/y_ts.shape[0])\n",
    "print('Log Avg Hamming: ', 100*ham_l.mean(), '%')\n",
    "print('Log Zero-One loss: ', 100*ham_l[ham_l==1].sum()/y_ts.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style  type=\"text/css\" >\n",
       "        \n",
       "        \n",
       "            #T_469cb238_427f_11e7_bda1_d05099aa5affrow2_col0 {\n",
       "            \n",
       "                color:  red;\n",
       "            \n",
       "            }\n",
       "        \n",
       "            #T_469cb238_427f_11e7_bda1_d05099aa5affrow2_col1 {\n",
       "            \n",
       "                color:  red;\n",
       "            \n",
       "            }\n",
       "        \n",
       "        </style>\n",
       "\n",
       "        <table id=\"T_469cb238_427f_11e7_bda1_d05099aa5aff\" None>\n",
       "        \n",
       "\n",
       "        <thead>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                \n",
       "                <th class=\"index_name level0\" >\n",
       "                  Model:\n",
       "                \n",
       "                \n",
       "                \n",
       "                <th class=\"col_heading level0 col0\" colspan=1>\n",
       "                  Average Hamming:\n",
       "                \n",
       "                \n",
       "                \n",
       "                <th class=\"col_heading level0 col1\" colspan=1>\n",
       "                  Zero-One:\n",
       "                \n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "        </thead>\n",
       "        <tbody>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                \n",
       "                <th id=\"T_469cb238_427f_11e7_bda1_d05099aa5aff\"\n",
       "                 class=\"row_heading level0 row0\" rowspan=1>\n",
       "                    SVC\n",
       "                \n",
       "                \n",
       "                \n",
       "                <td id=\"T_469cb238_427f_11e7_bda1_d05099aa5affrow0_col0\"\n",
       "                 class=\"data row0 col0\" >\n",
       "                    0.923\n",
       "                \n",
       "                \n",
       "                \n",
       "                <td id=\"T_469cb238_427f_11e7_bda1_d05099aa5affrow0_col1\"\n",
       "                 class=\"data row0 col1\" >\n",
       "                    0.48\n",
       "                \n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                \n",
       "                <th id=\"T_469cb238_427f_11e7_bda1_d05099aa5aff\"\n",
       "                 class=\"row_heading level0 row1\" rowspan=1>\n",
       "                    LogReg\n",
       "                \n",
       "                \n",
       "                \n",
       "                <td id=\"T_469cb238_427f_11e7_bda1_d05099aa5affrow1_col0\"\n",
       "                 class=\"data row1 col0\" >\n",
       "                    0.923\n",
       "                \n",
       "                \n",
       "                \n",
       "                <td id=\"T_469cb238_427f_11e7_bda1_d05099aa5affrow1_col1\"\n",
       "                 class=\"data row1 col1\" >\n",
       "                    0.49\n",
       "                \n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                \n",
       "                <th id=\"T_469cb238_427f_11e7_bda1_d05099aa5aff\"\n",
       "                 class=\"row_heading level0 row2\" rowspan=1>\n",
       "                    CRF\n",
       "                \n",
       "                \n",
       "                \n",
       "                <td id=\"T_469cb238_427f_11e7_bda1_d05099aa5affrow2_col0\"\n",
       "                 class=\"data row2 col0\" >\n",
       "                    0.954\n",
       "                \n",
       "                \n",
       "                \n",
       "                <td id=\"T_469cb238_427f_11e7_bda1_d05099aa5affrow2_col1\"\n",
       "                 class=\"data row2 col1\" >\n",
       "                    0.62\n",
       "                \n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "        </tbody>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "<pandas.formats.style.Styler at 0x7f2fb61507f0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hamm = [0.923, 0.923, 0.954]\n",
    "zo = [0.480, 0.490, 0.620]\n",
    "models = ['SVC', 'LogReg', 'CRF']\n",
    "met = ['Average Hamming', 'Zero-One']\n",
    "res_df = pd.DataFrame(hamm)\n",
    "res_df.columns = ['Average Hamming:']\n",
    "res_df['Zero-One:'] = zo\n",
    "res_df.columns.name = 'Model:'\n",
    "res_df.index = models\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['color: red' if v else '' for v in is_max]\n",
    "res_df.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0 . Current Average Test Hamming Accuracy:  92.4666666667 %.\n",
      "Iteration:  10 . Current Average Test Hamming Accuracy:  92.2666666667 %.\n",
      "Iteration:  20 . Current Average Test Hamming Accuracy:  92.4 %.\n",
      "Iteration:  30 . Current Average Test Hamming Accuracy:  92.4 %.\n",
      "Iteration:  40 . Current Average Test Hamming Accuracy:  92.4 %.\n",
      "Iteration:  50 . Current Average Test Hamming Accuracy:  92.4 %.\n"
     ]
    }
   ],
   "source": [
    "# Simultaneuos Pertubation Stochastic Approximation\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "alpha, gamma = 0.602, 0.101\n",
    "A, c, a = 5, 0.0001, 0.0001\n",
    "\n",
    "#crf_test3 = CRF(feat_map_toy,100 + 10**3,50, 10, 5)\n",
    "\n",
    "k = 0\n",
    "\n",
    "while k < 50:\n",
    "    \n",
    "    alpha_k = a / (A + k +1)**alpha\n",
    "    c_k = c / (k + 1)**gamma\n",
    "    del_k = 2* c_k * bernoulli.rvs(0.5,size=crf_test3.K) - c_k\n",
    "    crf_p = CRF(feat_map_toy,100 + 10**3,50, 10, 5, crf_test3.W + del_k)\n",
    "    crf_m = CRF(feat_map_toy,100 + 10**3,50, 10, 5, crf_test3.W - del_k)\n",
    "    \n",
    "    diff = (crf_p.reg_neg_ll(X_tr,y_tr) - crf_m.reg_neg_ll(X_tr,y_tr)) / 2\n",
    "    crf_test3.W = crf_test3.W - (alpha_k * diff) * (1 / del_k)\n",
    "    if k  % 10 == 0:\n",
    "            \n",
    "        acc_test = np.zeros(30)\n",
    "            \n",
    "        for n in range(30):\n",
    "            acc_test[n] = 1 - hamming(crf_test3.MAP(X_ts[n]), y_ts[n])\n",
    "            \n",
    "        print('Iteration: ',k, '. Current Average Test Hamming Accuracy: ',\n",
    "            100* acc_test.mean(), '%.')\n",
    "        \n",
    "        \n",
    "    k += 1\n",
    "\n",
    "acc_test = np.zeros(30)\n",
    "            \n",
    "for n in range(30):\n",
    "    acc_test[n] = 1 - hamming(crf_test3.MAP(X_ts[n]), y_ts[n])\n",
    "            \n",
    "print('Iteration: ',k, '. Current Average Test Hamming Accuracy: ',\n",
    "        100* acc_test.mean(), '%.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1, -1,  1, -1, -1, -1, -1,  1])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*bernoulli.rvs(0.5,size=10) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26569467,  0.42955854, -0.32809504, -0.66751197, -0.21377015,\n",
       "        0.01204941,  0.48927196, -0.12374394, -0.42415367,  0.26093447,\n",
       "        0.05492973,  0.67446071,  0.62202448,  0.45847976,  0.5953965 ,\n",
       "       -0.42086594, -0.53643886, -0.22221041, -0.38168191,  0.30985493,\n",
       "        0.3494285 ,  0.17120865,  0.22322946, -0.68989205, -0.14076703,\n",
       "        0.63953407, -0.26233943,  0.12768074, -0.61103329, -0.04919524,\n",
       "       -0.8038806 , -0.01245953,  0.48799304,  0.1842084 , -0.19853262,\n",
       "       -0.1131701 , -0.71577879, -0.12735687, -0.20848707, -0.03354356,\n",
       "       -0.60792553,  0.43303068,  0.17809504, -0.27085521, -0.40819822,\n",
       "       -0.24451328, -0.09458871,  0.39832734, -0.25120413,  0.24378356,\n",
       "        0.35284519, -0.10262145,  0.06500082, -0.12969668,  0.35326028,\n",
       "       -0.28181343,  0.51375231,  0.19135779,  0.08392776, -0.20091778,\n",
       "       -0.23038371,  0.01619796, -0.06379931,  0.0260595 ,  0.07310725,\n",
       "        0.15199807,  0.57456157,  0.04200584, -0.00782348, -0.11081072,\n",
       "       -0.46576896, -0.24885288,  0.09284354,  0.21389617, -0.06689812,\n",
       "       -0.6910673 , -0.15195421,  0.05161979, -0.1061767 ,  0.2344766 ,\n",
       "       -0.53134289,  0.18369387,  0.32595765,  0.21984503, -0.46228221,\n",
       "        0.44698801, -0.36938263,  0.65520707, -0.58541143, -0.29606008,\n",
       "       -0.07571372,  0.2037571 , -0.03599729, -0.54689611,  0.03717086,\n",
       "       -0.28683645, -0.2798002 , -0.03387981, -0.01362575, -0.077406  ,\n",
       "        0.13991165,  0.54352276,  0.06893001, -0.15000671,  0.02135578,\n",
       "       -0.65566358,  0.15898961, -0.42377819, -0.0675417 ,  0.33744833,\n",
       "       -0.09966381, -0.41959777, -0.39247606, -0.00817515,  0.30821472,\n",
       "       -0.02278938,  0.50415013, -0.05039492,  0.93344001,  0.45126287,\n",
       "        0.11440299, -0.11490458,  0.46541292, -0.00481902,  0.05188555,\n",
       "       -0.14337681, -0.39046848, -0.1666009 , -0.55793642,  0.59016504,\n",
       "        0.24533687,  0.31014084,  0.04764331,  0.47074719,  0.43431262,\n",
       "       -0.19291241, -0.37055044,  0.57594629, -0.01096798,  0.06100604,\n",
       "        0.02893705, -0.41638993, -0.25812692, -0.65326949,  0.37128866,\n",
       "       -0.18129274, -0.48720713, -0.41221965, -0.09744742,  1.16421438])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_test3.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 0)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_test3.MAP(X_f[-16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_f[-16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.86666667,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  0.93333333,  1.        ,  1.        ,\n",
       "        1.        ,  0.86666667,  1.        ,  0.93333333,  0.93333333,\n",
       "        0.8       ,  0.4       ,  1.        ,  1.        ,  1.        ,\n",
       "        0.93333333,  0.86666667,  1.        ,  1.        ,  0.8       ,\n",
       "        0.8       ,  0.86666667,  1.        ,  0.8       ,  0.86666667,\n",
       "        0.93333333,  1.        ,  1.        ,  0.86666667,  1.        ,\n",
       "        1.        ,  0.93333333,  0.8       ,  1.        ,  1.        ,\n",
       "        0.93333333,  0.86666667,  1.        ,  1.        ,  0.8       ,\n",
       "        1.        ,  0.8       ,  1.        ,  0.93333333,  0.46666667,\n",
       "        0.86666667,  1.        ,  1.        ,  0.93333333,  0.93333333,\n",
       "        1.        ,  1.        ,  0.93333333,  0.86666667,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  0.73333333,  1.        ,\n",
       "        0.93333333,  0.93333333,  0.93333333,  1.        ,  1.        ,\n",
       "        1.        ,  0.8       ,  0.93333333,  0.86666667,  0.73333333,\n",
       "        1.        ,  1.        ,  0.66666667,  0.86666667,  0.8       ,\n",
       "        0.93333333,  1.        ,  1.        ,  0.86666667,  0.93333333,\n",
       "        1.        ,  1.        ,  0.93333333,  1.        ,  0.86666667,\n",
       "        1.        ,  1.        ,  0.73333333,  0.93333333,  1.        ,\n",
       "        0.93333333,  0.93333333,  1.        ,  0.93333333,  0.8       ,\n",
       "        0.4       ,  0.93333333,  0.86666667,  0.93333333,  1.        ,\n",
       "        1.        ,  0.66666667,  1.        ,  0.8       ,  1.        ,\n",
       "        0.86666667,  1.        ,  0.93333333,  0.93333333,  1.        ,\n",
       "        0.66666667,  1.        ,  0.86666667,  0.93333333,  1.        ,\n",
       "        0.93333333,  1.        ,  0.86666667,  0.93333333,  0.73333333,\n",
       "        0.93333333,  0.8       ,  0.86666667,  0.93333333,  0.93333333,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  0.93333333,\n",
       "        0.93333333,  1.        ,  0.93333333,  1.        ,  0.93333333,\n",
       "        0.86666667,  1.        ,  0.8       ,  0.86666667,  1.        ,\n",
       "        0.86666667,  0.93333333,  1.        ,  0.8       ,  0.8       ,\n",
       "        1.        ,  0.86666667,  0.86666667,  0.53333333,  0.93333333,\n",
       "        0.66666667,  1.        ,  0.93333333,  1.        ,  0.86666667,\n",
       "        0.8       ,  0.93333333,  1.        ,  0.93333333,  1.        ,\n",
       "        1.        ,  0.93333333,  0.66666667,  0.8       ,  1.        ,\n",
       "        0.93333333,  0.8       ,  1.        ,  0.93333333,  0.86666667,\n",
       "        1.        ,  1.        ,  1.        ,  0.73333333,  0.93333333,\n",
       "        1.        ,  1.        ,  1.        ,  0.66666667,  1.        ,\n",
       "        1.        ,  0.93333333,  1.        ,  0.53333333,  0.8       ,\n",
       "        0.8       ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  0.93333333,  1.        ,  1.        ,  0.93333333,\n",
       "        1.        ,  0.93333333,  1.        ,  0.93333333,  0.93333333,\n",
       "        0.86666667,  1.        ,  0.86666667,  0.73333333,  1.        ,\n",
       "        0.8       ,  0.86666667,  0.93333333,  1.        ,  0.6       ,\n",
       "        1.        ,  0.86666667,  0.93333333,  1.        ,  0.93333333,\n",
       "        0.86666667,  0.66666667,  1.        ,  0.93333333,  0.93333333,\n",
       "        0.8       ,  0.73333333,  1.        ,  1.        ,  0.86666667,\n",
       "        1.        ,  1.        ,  0.86666667,  1.        ,  0.8       ,\n",
       "        1.        ,  1.        ,  0.8       ,  0.8       ,  0.93333333,\n",
       "        1.        ,  1.        ,  0.93333333,  1.        ,  0.8       ,\n",
       "        1.        ,  1.        ,  0.73333333,  0.93333333,  0.93333333,\n",
       "        0.73333333,  0.93333333,  1.        ,  0.8       ,  0.86666667,\n",
       "        0.8       ,  0.8       ,  0.86666667,  0.8       ,  0.93333333,\n",
       "        0.93333333,  1.        ,  0.93333333,  1.        ,  0.86666667,\n",
       "        1.        ,  0.86666667,  1.        ,  0.8       ,  0.93333333,\n",
       "        0.73333333,  1.        ,  0.86666667,  0.73333333,  1.        ,\n",
       "        0.93333333,  1.        ,  0.73333333,  0.66666667,  1.        ,\n",
       "        1.        ,  0.93333333,  1.        ,  0.8       ,  0.6       ,\n",
       "        0.8       ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        0.86666667,  0.93333333,  0.93333333,  0.93333333,  0.8       ,\n",
       "        0.86666667,  0.93333333,  0.86666667,  0.73333333,  1.        ])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
