{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from timeit import default_timer as dft\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balanced_error_rate(y_true, y_test):\n",
    "    \n",
    "    conf = confusion_matrix(y_true, y_test)\n",
    "    \n",
    "    return 0.5 * (conf[0,1] / conf[0].sum()) + 0.5 * (conf[1,0] / conf[1].sum())\n",
    "\n",
    "def error_metrics(y_true, y_test):\n",
    "    \n",
    "    print('BER: ', balanced_error_rate(y_true, y_test))\n",
    "    print('F1: ', f1_score(y_true, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    '''data_name:  'arcene', 'dorothea', 'gisette', 'dexter', or 'madelon'. '''\n",
    "    \n",
    "    fp = '/home/james/anaconda3/data/' + data_name.upper() + '/'\n",
    "    \n",
    "    if data_name not in ['dorothea', 'dexter']:\n",
    "        \n",
    "        X_train = np.loadtxt(fp + data_name + '_train.data')\n",
    "        y_train = np.loadtxt(fp + data_name + '_train.labels')\n",
    "        X_test = np.loadtxt(fp + data_name + '_valid.data')\n",
    "        y_test = np.loadtxt(fp + data_name + '_valid.labels')\n",
    "        y_train = np.asanyarray((y_train + 1) // 2, dtype = 'i8')\n",
    "        y_test = np.asanyarray((y_test + 1) // 2, dtype = 'i8')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        X_train = np.load(fp + data_name + '_train.npy')\n",
    "        y_train = np.loadtxt(fp + data_name + '_train.labels')\n",
    "        X_test = np.load(fp + data_name + '_valid.npy')\n",
    "        y_test = np.loadtxt(fp + data_name + '_valid.labels')\n",
    "        y_train = np.asanyarray((y_train + 1) // 2, dtype = 'i8')\n",
    "        y_test = np.asanyarray((y_test + 1) // 2, dtype = 'i8')\n",
    "        \n",
    "        \n",
    "    print(data_name.upper() + ' Data:')\n",
    "    print('Training data info: ')\n",
    "    print(X_train.shape[0], ' samples, ', X_train.shape[1], ' features.')\n",
    "    print(int(y_train.sum()), ' positve samples, ', int(X_train.shape[0] - y_train.sum()), ' negative samples')\n",
    "    print('-------------------------------')\n",
    "    print('Test data info: ')\n",
    "    print(X_test.shape[0], ' samples.')\n",
    "    print(int(y_test.sum()), ' positve samples, ', int(X_test.shape[0] - y_test.sum()), ' negative samples')\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Particle(object):\n",
    "    \n",
    "    def __init__(self, tot_feat, c_s='def'):\n",
    "        \n",
    "        if c_s == 'def':\n",
    "            \n",
    "            choice_size = np.random.choice(np.arange(1,tot_feat))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            choice_size = np.random.choice(np.arange(1,c_s))\n",
    "            \n",
    "        self.locs = np.sort(np.random.choice(np.arange(tot_feat),\n",
    "                                             size=choice_size,\n",
    "                                             replace=False))\n",
    "        self.p = np.zeros(tot_feat, dtype='i8')\n",
    "        self.v = np.random.uniform(low=-7, high=7, size=tot_feat)\n",
    "        self.p[self.locs] = np.ones(len(self.locs), dtype='i8')\n",
    "        self.num_feats = int(self.p.sum())\n",
    "        self.p_best = np.copy(self.p)\n",
    "        self.ber_best = 1\n",
    "        self.ber = None\n",
    "        self.f1 = None\n",
    "        self.auc = None\n",
    "        \n",
    "    def fit_clf(self, clf, size_reg,  X_train, y_train):\n",
    "        \n",
    "        clf.fit(X_train[:,self.locs], y_train)\n",
    "        self.ber = (balanced_error_rate(y_test, clf.predict(X_test[:,self.locs])) +\n",
    "                   0 * (np.copy(self.num_feats) / len(self.p)))\n",
    "        self.f1 = f1_score(y_test, clf.predict(X_test[:,self.locs]))\n",
    "        self.auc = roc_auc_score(y_test, clf.predict(X_test[:,self.locs]))\n",
    "        \n",
    "        if self.ber + 0 * (self.num_feats / len(self.p)) < self.ber_best:\n",
    "            \n",
    "            self.p_best = np.copy(self.p)\n",
    "            self.ber_best = np.copy(self.ber) + size_reg * (np.copy(self.num_feats) / len(self.p))\n",
    "            \n",
    "    def update_locs(self):\n",
    "        \n",
    "        self.locs = np.where(self.p == 1)[0]\n",
    "        self.num_feats = int(self.p.sum())\n",
    "        \n",
    "def update_velocity(part, g_best_pos):\n",
    "    \n",
    "    r1, r2 = np.random.uniform(), np.random.uniform()\n",
    "    part.v = np.clip(part.v + (3 * r1 * (part.p_best - part.p))  + (2 * r2 * (g_best_pos - part.p)),\n",
    "                     -10, 10)\n",
    "    \n",
    "def logistic(v):\n",
    "    \n",
    "    return (1 + np.exp(-v))**(-1)\n",
    "\n",
    "def update_position(part):\n",
    "    \n",
    "    S = logistic(part.v)\n",
    "    r = np.random.uniform()\n",
    "    new_p = np.asanyarray(S >= r, dtype='i8')\n",
    "    \n",
    "    if new_p.sum() != 0:\n",
    "        \n",
    "        part.p = new_p\n",
    "        \n",
    "    part.update_locs()\n",
    "    \n",
    "\n",
    "def run_pso(swarm_size, num_iter, size_reg, clf, itr=1):\n",
    "    \n",
    "    print('Initializing the swarm....')\n",
    "    \n",
    "    swarm = []\n",
    "\n",
    "    for k in range(swarm_size):\n",
    "    \n",
    "        swarm.append(Particle(X_train.shape[1]))\n",
    "    \n",
    "    swarm[0].fit_clf(clf, size_reg, X_train, y_train)\n",
    "\n",
    "    g_best_loc = 0\n",
    "    g_best_pos = np.copy(swarm[0].p)\n",
    "    g_best_score = np.copy(swarm[0].ber) + size_reg * (np.copy(swarm[0].num_feats) / len(swarm[0].p))\n",
    "\n",
    "    for k in range(1,len(swarm)):\n",
    "    \n",
    "        swarm[k].fit_clf(clf, size_reg,  X_train, y_train)\n",
    "    \n",
    "        if g_best_score > swarm[k].ber + size_reg * (swarm[k].num_feats / len(swarm[k].p)):\n",
    "        \n",
    "            g_best_loc = k\n",
    "            g_best_pos = np.copy(swarm[k].p)\n",
    "            g_best_score = np.copy(swarm[k].ber) + size_reg * (np.copy(swarm[k].num_feats) / len(swarm[k].p))\n",
    "            \n",
    "    print('Iteration ', itr, ' Initial Best (BER, f1, num_feats, score): (', swarm[g_best_loc].ber,\n",
    "                  ', ', swarm[g_best_loc].f1, ', ', swarm[g_best_loc].num_feats,', ', g_best_score, ')')\n",
    "\n",
    "    for iteration in tqdm(range(num_iter)):\n",
    "    \n",
    "        for k in range(len(swarm)):\n",
    "        \n",
    "            update_velocity(swarm[k], g_best_pos)\n",
    "            update_position(swarm[k])\n",
    "            swarm[k].fit_clf(clf, size_reg, X_train, y_train)\n",
    "    \n",
    "        for k in range(len(swarm)):\n",
    "    \n",
    "            if g_best_score > swarm[k].ber + size_reg * (swarm[k].num_feats / len(swarm[k].p)):\n",
    "            \n",
    "                g_best_loc = k\n",
    "                g_best_pos = np.copy(swarm[k].p)\n",
    "                g_best_score = np.copy(swarm[k].ber) + size_reg * (np.copy(swarm[k].num_feats) / len(swarm[k].p))\n",
    "                print(' Iteration ', itr, ' Current Best (BER, f1, num_feats, score): (', swarm[k].ber,\n",
    "                  ', ', swarm[k].f1, ', ', swarm[k].num_feats,', ', g_best_score, ')')\n",
    "                \n",
    "        b_feat = int(min(len(swarm[0].p), 2 * swarm[g_best_loc].num_feats))\n",
    "        rand_ind = np.random.choice(np.arange(swarm_size, dtype='i8'), replace=False, size=15)\n",
    "        \n",
    "        for ind in rand_ind:\n",
    "            \n",
    "            if ind != g_best_loc:\n",
    "                \n",
    "                swarm[ind] = Particle(X_train.shape[1], b_feat)\n",
    "                swarm[ind].fit_clf(clf, size_reg, X_train, y_train)\n",
    "            \n",
    "    best_part = Particle(X_train.shape[-1])\n",
    "    best_part.p = g_best_pos\n",
    "    best_part.update_locs()\n",
    "    best_part.fit_clf(clf, size_reg, X_train, y_train)\n",
    "    \n",
    "    return best_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "plt.rc(\"font\", size=14)\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "\n",
    "def train(x_train,y_train):\n",
    "    logreg = LogisticRegression(class_weight='balanced', C=0.0001)\n",
    "    logreg.fit(x_train, y_train)\n",
    "    return logreg\n",
    "\n",
    "def get_ber(y_pred, y_test):\n",
    "    ber = (np.mean(y_pred[y_test == 0] != 0 )+np.mean(y_pred[y_test == 1] != 1 ))/2\n",
    "    return ber\n",
    "\n",
    "def get_metrics(x_test, y_test, logreg):\n",
    "    print('Accuracy: ', logreg.score(x_test, y_test))\n",
    "    y_pred = logreg.predict(x_test)\n",
    "    print('BER:', get_ber(y_pred, y_test))\n",
    "    print(sklearn.metrics.classification_report(y_pred, y_test))\n",
    "    print('Confusion Matrix\\n', sklearn.metrics.confusion_matrix(y_pred, y_test))\n",
    "    \n",
    "    \n",
    "def get_cost(x, y, x_test, y_test):\n",
    "    #Train or test TODO: decide\n",
    "    #mean = x.mean(axis=0) \n",
    "    #std = x.std(axis=0)+1e-6\n",
    "    #x_n = (x - mean) / std\n",
    "    #logreg = train(x_n, y)\n",
    "    logreg = train(x, y)\n",
    "    \n",
    "    #x_test_n = (x_test - mean) / std\n",
    "    #y_pred = logreg.predict(x_test_n)\n",
    "    y_pred = logreg.predict(x_test)\n",
    "    \n",
    "    n = x_test.shape[0]\n",
    "    #aic = n*np.log(sum((y_pred-y_test)*(y_pred-y_test))/n)+2*(x_test.shape[1]+1)\n",
    "    #AIC=n∗ln(SSEn)+2k\n",
    "    ber = get_ber(y_pred, y_test)\n",
    "    \n",
    "    return ber\n",
    "\n",
    "\n",
    "def get_rcost(x_train, y_train, x_test, y_test, ind):\n",
    "    cost =  get_cost(x_train[:,ind==1],y_train, x_test[:, ind==1], y_test)\n",
    "    \n",
    "    #return cost+0.001*np.mean(ind)\n",
    "    return cost+0.0001*np.sum(ind)\n",
    "    #return cost\n",
    "\n",
    "nbr=6\n",
    "\n",
    "def rand_init(x_train, y_train, n=50000, seed=123456):\n",
    "    size = x_train.shape[1]\n",
    "    ind = np.zeros(size)\n",
    "    a = np.arange(size)\n",
    "    np.random.shuffle(a)\n",
    "    ind[a[:n]] = 1\n",
    "    return ind\n",
    "\n",
    "def get_nbr(ind, code=1, nbr=2, seed=123456):\n",
    "    #print(\"Code\", code, \"bits\", nbr)\n",
    "    if code == 1:\n",
    "        #This part looks for random 1/2 position swap\n",
    "        return rand_swap_nbr(ind, nbr, seed=123456)\n",
    "    elif code == 2:\n",
    "        #This part allows grow/shrink\n",
    "        return rand_any_nbr(ind, nbr, seed=123456)\n",
    "    elif code == 3:\n",
    "        #This part allows grow/shrink\n",
    "        return rand_mutate_nbr(ind, nbr, seed=123456)\n",
    "    else:\n",
    "        print('Wrong code')\n",
    "    return ind\n",
    "\n",
    "\n",
    "\n",
    "def get_rand_nbr(ind, code=1, nbr=30, bits=2):\n",
    "    #print(\"code\", code, \"nbr\",nbr)\n",
    "    ind_nbr = np.zeros((nbr, ind.shape[0]))\n",
    "    for i in range(0, nbr):\n",
    "        ind_nbr[i] = get_nbr(ind, code, nbr=bits)\n",
    "      \n",
    "    return ind_nbr\n",
    "\n",
    "def rand_swap_nbr(ind, nbr=2, seed=123456):\n",
    "    '''\n",
    "    ind --> is a 0/1 array. Neighbours of this array would be maximum of any two positions changed\n",
    "    '''\n",
    "    ind_pos, = np.where(ind==1)\n",
    "    ind_neg, = np.where(ind==0)\n",
    "    \n",
    "    \n",
    "    pos = np.random.choice(ind_pos, nbr/2, replace=False)\n",
    "    ind[pos] = 0\n",
    "     \n",
    "    neg = np.random.choice(ind_neg, nbr/2, replace=False)\n",
    "    ind[neg] = 1\n",
    "    return ind\n",
    "\n",
    "\n",
    "def rand_any_nbr(ind, nbr=2, seed=123456):\n",
    "    '''\n",
    "    ind --> is a 0/1 array. Neighbours of this array would be maximum of any two positions changed\n",
    "    '''\n",
    "    ind_pos, = np.where(ind==1)\n",
    "    ind_neg, = np.where(ind==0)\n",
    "        \n",
    "    ch = np.random.uniform(0,2)\n",
    "    if round(ch) == 0:\n",
    "        #change some 1 to 0\n",
    "        n = round(np.random.uniform(1,nbr))\n",
    "        ind[np.random.choice(ind_pos, n)] = 0\n",
    "    elif round(ch) == 1:\n",
    "        n1 = round(np.random.uniform(1,nbr))\n",
    "        ind[np.random.choice(ind_pos, n1)] = 0\n",
    "        n2 = round(np.random.uniform(1,nbr))\n",
    "        ind[np.random.choice(ind_neg, n2)] = 1\n",
    "        \n",
    "    else:\n",
    "        n = round(np.random.uniform(1,nbr))\n",
    "        ind[np.random.choice(ind_neg, n)] = 1\n",
    "    \n",
    "    return ind\n",
    "\n",
    "def rand_mutate_nbr(ind, nbr=2, seed=123456):\n",
    "    '''\n",
    "    ind --> is a 0/1 array. Neighbours of this array would be maximum of any two positions changed\n",
    "    '''\n",
    "        \n",
    "    D = ind.shape[0]\n",
    "    \n",
    "    #new neighbor\n",
    "    ind1 = np.zeros(D)\n",
    "    a = np.arange(D)\n",
    "    np.random.shuffle(a)\n",
    "    start_n = np.random.randint(D)\n",
    "    ind1[a[:start_n]] = 1\n",
    "    \n",
    "    #Mutate\n",
    "    n = np.random.randint(2, size=D)\n",
    "    n_orig, = np.where(n==1)\n",
    "    n_new, = np.where(n==0)\n",
    "    \n",
    "    ind2 = np.zeros(D)\n",
    "    ind2[n_orig] = ind[n_orig]\n",
    "    ind2[n_new] = ind1[n_new]\n",
    "    \n",
    "    return ind2\n",
    "\n",
    "def rand_mutate_nbr1(ind, nbr=2, seed=123456):\n",
    "    '''\n",
    "    ind --> is a 0/1 array. Neighbours of this array would be maximum of any two positions changed\n",
    "    '''\n",
    "        \n",
    "    D = ind.shape[0]\n",
    "    a = np.arange(D)\n",
    "    rand_ind = np.random.choice(a, nbr)\n",
    "    \n",
    "    \n",
    "    #Generate a 0/1 for rand choice\n",
    "    ind_phi = np.random.randint(2, size=nbr)\n",
    "    phi_pos, = np.where(ind_phi==1)\n",
    "    phi_neg, = np.where(ind_phi==0)\n",
    "    \n",
    "    #print(phi_pos, phi_neg)\n",
    "    #print(rand_ind, rand_ind[phi_pos], rand_ind[phi_neg])\n",
    "    ind[rand_ind[phi_pos]]=1\n",
    "    ind[rand_ind[phi_neg]]=0\n",
    "    '''\n",
    "    j = 0\n",
    "    for i in rand_ind:\n",
    "        if ind_phi[j] == 0:\n",
    "            if ind[i] == 1:\n",
    "                ind[i] = 0 \n",
    "                #flip the selection\n",
    "        else:\n",
    "            if ind[i] == 0:\n",
    "                ind[i] = 1 \n",
    "                #flip the selection\n",
    "        j=j+1\n",
    "    '''\n",
    "    return ind\n",
    "\n",
    "\n",
    "import os\n",
    "import errno\n",
    "\n",
    "def  save_res(save_path='results', file='Temp', ind=[], ber=0.0,c_a=[]):\n",
    "\n",
    "    try:\n",
    "        os.makedirs (save_path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "        \n",
    "\n",
    "    np.savez_compressed(os.path.join(save_path, file),\n",
    "                        ind = ind, ber=ber, cost_arr=c_a)        \n",
    "\n",
    "\n",
    "np.random.seed(123456)\n",
    "\n",
    "def initial_temp():\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "def acceptance_probability(old_cost, new_cost, temp):\n",
    "    return np.exp((old_cost - new_cost) / temp)\n",
    "\n",
    "\n",
    "def accept_proposal(current, proposal, temperature):\n",
    "    \n",
    "    if (proposal < current):\n",
    "        return 1\n",
    "    if (temperature == 0.0):\n",
    "        return 0\n",
    "    prob = np.exp(-(proposal - current) / temperature)\n",
    "    return np.random.uniform() < prob\n",
    "\n",
    "\n",
    "def reduce_temp(T, repetitions, code=1):\n",
    "    alpha = 0.9\n",
    "    if code == 1:\n",
    "        return T*alpha\n",
    "    elif code == 2:\n",
    "        T=T/math.log(repetitions+2,2)\n",
    "        #print('New temp:',T)\n",
    "        return T\n",
    "    else: \n",
    "        T = T - 0.1\n",
    "        return T\n",
    "\n",
    "    \n",
    "def run_SA(x_train, y_train, x_test, y_test, max_iter=1000, nbr=30, bits=4, nbr_code=3, temp_code=2):\n",
    "\n",
    "    tol = 0.0001\n",
    "    #Initialize\n",
    "    cost_arr = np.zeros((max_iter*2,2))\n",
    "    start_n = np.random.randint(x_train.shape[1])\n",
    "    ind = rand_init(x_train, y_train, start_n, seed=123456)\n",
    "    #cost = get_rcost(x_train, y_train, x_test, y_test, ind)\n",
    "    cost = get_cost(x_train[:,ind==1], y_train, x_test[:,ind==1], y_test)\n",
    "    print('Initial BER: ',cost)\n",
    "    cost_arr[0,0] = cost\n",
    "    cost_arr[0,1] = sum(ind)\n",
    "    #Move\n",
    "    ii = 0\n",
    "    T = initial_temp( )\n",
    "    T_min = 0.0001\n",
    "    \n",
    "    while ii < max_iter and T > T_min:\n",
    "        j = 0\n",
    "        while j <= min(100, max_iter):\n",
    "            ind_nbr = get_rand_nbr(ind, nbr_code, nbr=nbr, bits=4)\n",
    "        \n",
    "            ind1 = ind_nbr[0]\n",
    "            #cost1 = get_rcost(x_train,y_train, x_test, y_test, ind1)\n",
    "            cost1 = get_cost(x_train[:,ind1==1],y_train, x_test[:,ind1==1], y_test)\n",
    "            for i in range(1, nbr):   \n",
    "                #cost_i = get_rcost(x_train,y_train, x_test, y_test, ind_nbr[i])\n",
    "                cost_i = get_cost(x_train[:, ind_nbr[i]==1],y_train, x_test[:, ind_nbr[i]==1], y_test )\n",
    "                if cost_i < cost1:\n",
    "                    cost1 = cost_i\n",
    "                    ind1 = ind_nbr[i]\n",
    "            \n",
    "            #ap = acceptance_probability(cost, cost1, T)\n",
    "            #if ap > np.random.uniform():\n",
    "            ap = accept_proposal(cost, cost1, T)\n",
    "            if ap == 1:\n",
    "                ind = ind1\n",
    "                cost = cost1\n",
    "                #print(cost,end=' ')\n",
    "                #print('i',ii, 'j',j)\n",
    "                #print(cost, ii, end=' ')\n",
    "            j += 1\n",
    "           \n",
    "            cost_arr[ii,0] = cost\n",
    "            cost_arr[ii,1] = sum(ind)\n",
    "            ii = ii+1\n",
    "            print(\".\",end='')\n",
    "            if ii%100 == 0:\n",
    "                #print(sum(ind), cost)\n",
    "                print(cost, ii)\n",
    "            \n",
    "        T = reduce_temp(T, ii, temp_code)\n",
    "        #print(i)\n",
    "    \n",
    "    print('Features selected:', sum(ind))\n",
    "    print('Final cost', cost)\n",
    "    \n",
    "    print('Iterations run', ii,'\\nFinal cost ',cost)\n",
    "    logreg = train(x_train[:,ind==1], y_train)\n",
    "    #get_test_acc(x_test[:,ind==1], y_test, logreg)\n",
    "    print(logreg.score(x_test[:,ind==1], y_test))\n",
    "    \n",
    "    return ind, cost, cost_arr\n",
    "\n",
    "\n",
    "def run_HC(x_train, y_train, x_test, y_test, max_iter=500, restart_iter=200, nbr=3, bits=6, nbr_code=3):\n",
    "\n",
    "    tol = 0.0001\n",
    "\n",
    "    #Initialize\n",
    "    c_a = np.zeros((max_iter,2))\n",
    "    start_n = np.random.randint(x_train.shape[1])\n",
    "    ind = rand_init(x_train, y_train, start_n, seed=123456)\n",
    "    print(x_train[:,ind==1].shape)\n",
    "\n",
    "    #cost = get_rcost(x_train, y_train, x_test, y_test, ind)\n",
    "    cost = get_cost(x_train[:, ind==1], y_train, x_test[:, ind==1], y_test)\n",
    "    print('Initial Cost: ',cost)\n",
    "    c_a[0,0] = cost\n",
    "    c_a[0,1] = sum(ind)\n",
    "\n",
    "    #Move\n",
    "    update = 0\n",
    "    for i in range(0, max_iter):\n",
    "        #Get random neighbors\n",
    "        ind_nbr = get_rand_nbr(ind, nbr_code, nbr=nbr, bits=bits)\n",
    "    \n",
    "        ind1 = ind_nbr[0]\n",
    "        #cost1 = get_rcost(x_train, y_train, x_test, y_test, ind1)\n",
    "        cost1 = get_cost(x_train[:, ind1==1], y_train, x_test[:, ind1==1], y_test)\n",
    "    \n",
    "        for ii in range(1, nbr):   \n",
    "            #cost_i = get_rcost(x_train,y_train, x_test, y_test, ind_nbr[ii])\n",
    "            cost_i = get_cost(x_train[:,ind_nbr[ii]==1],y_train, x_test[:,ind_nbr[ii]==1], y_test)\n",
    "            if cost_i < cost1:\n",
    "                cost1 = cost_i\n",
    "                ind1 = ind_nbr[ii]\n",
    "        #print(acc1)\n",
    "        if cost1 <  cost:\n",
    "            ind = ind1\n",
    "            cost = cost1\n",
    "            update += 1\n",
    "            \n",
    "            \n",
    "            print(cost,update,end=' ')\n",
    "        else:\n",
    "            print('.',end='')\n",
    "\n",
    "        c_a[i,0] = cost\n",
    "        c_a[i,1] = sum(ind)\n",
    "        if i%100 == 0:\n",
    "            print('\\n')\n",
    "            print(sum(ind))\n",
    "            #print(ind[:50])\n",
    "            print(cost)\n",
    "        \n",
    "        if i+1% restart_iter == 0:\n",
    "            if update == 0:\n",
    "                print('Restarting.......')\n",
    "                start_n = np.random.randint(x_train.shape[1])\n",
    "                ind = rand_init(x_train, y_train, n=start_n, seed=123456)\n",
    "                cost = get_cost(x_train[:,ind==1],y_train, x_test[:, ind==1], y_test)\n",
    "                init_cost = cost\n",
    "                update = 0\n",
    "                c_a[i,0] = cost\n",
    "                c_a[i,1] = sum(ind)\n",
    "        \n",
    "        #print(i)\n",
    "    print('Features selected:', sum(ind))\n",
    "    print('Final cost', cost)\n",
    "    print(ind[:50])\n",
    "\n",
    "    return ind, cost, c_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "from operator import attrgetter\n",
    "\n",
    "\n",
    "\n",
    "def balanced_error_rate(y_true, y_test):  \n",
    "    conf = confusion_matrix(y_true, y_test)\n",
    "    #print conf\n",
    "    return 0.5 * (conf[0,1] / conf[0].sum()) + 0.5 * (conf[1,0] / conf[1].sum())\n",
    "def full_feature(train_data,train_label,valid_data,valid_label):\n",
    "    models = []\n",
    "    models.append(('Logistic Regression', LogisticRegression(C=0.0001,class_weight='balanced')))\n",
    "    for name, model in models:\n",
    "        model.fit(train_data,train_label)\n",
    "        valid_accuracy = accuracy_score(valid_label, model.predict(valid_data))\n",
    "        #print valid_accuracy\n",
    "        #print valid_label\n",
    "        #print model.predict(valid_data_reduced)\n",
    "        BER = balanced_error_rate(valid_label,model.predict(valid_data))\n",
    "        #print BER\n",
    "        fitness = 1.0/BER\n",
    "    print('Full features BER')\n",
    "    print(BER)\n",
    "    return BER\n",
    "class individual:\n",
    "    def __init__(self,dim,train_data, train_label,valid_data,valid_label):\n",
    "        # every invidual is a vector, with the same dimension as the dataset\n",
    "        # Every dimension receive value 1 if that feature is used and 0 otherwise\n",
    "        # ind: vector contains 0,1 value\n",
    "        # dim: dimension (number of features)\n",
    "        # BER: balance error rate\n",
    "        # Probability: probability to choose this individual\n",
    "        \n",
    "        self.ind = self.create_individual(dim)       \n",
    "        self.fitness,self.BER,self.valid_accuracy = self.fitness_evaluation(self.ind,\n",
    "                                                train_data, train_label,valid_data,valid_label,dim)\n",
    "        self.probability = 0.0\n",
    "        self.dim = dim\n",
    "    def create_individual(self,dim):\n",
    "        choice_size = np.random.choice(np.arange(1,dim))\n",
    "        #on_position is position that have value 1, mean the features is chosen\n",
    "        on_position = np.sort(np.random.choice(np.arange(dim),size=choice_size,replace=False))\n",
    "        individual = np.zeros(dim, dtype='i8')\n",
    "        individual[on_position] = np.ones(len(on_position), dtype='i8')\n",
    "        return individual\n",
    "    def fitness_evaluation(self,individual, train_data,train_label,valid_data,valid_label,dim):\n",
    "        models = []\n",
    "        models.append(('Logistic Regression', LogisticRegression(C=0.0001,class_weight='balanced')))\n",
    "        fil = np.nonzero(individual)\n",
    "        train_data_reduced = train_data[:,fil[0]]\n",
    "        valid_data_reduced = valid_data[:,fil[0]]\n",
    "        for name, model in models:\n",
    "            model.fit(train_data_reduced, train_label)\n",
    "            valid_accuracy = accuracy_score(valid_label, model.predict(valid_data_reduced))\n",
    "            #print valid_accuracy\n",
    "            #print valid_label\n",
    "            #print model.predict(valid_data_reduced)\n",
    "            BER = balanced_error_rate(valid_label,model.predict(valid_data_reduced))\n",
    "            #print BER\n",
    "            fitness = 1.0/(BER +  0.001 * sum(self.ind)/dim)\n",
    "        return fitness,BER,valid_accuracy\n",
    "    def update_probability(self,total_fitness):\n",
    "        self.probability = self.fitness / total_fitness\n",
    "class population:\n",
    "    def __init__(self,population_size,dim,train_data,train_label,valid_data,valid_label):\n",
    "        self.pop = self.create_population(population_size,dim,train_data,train_label,valid_data,valid_label)\n",
    "        self.total_fitness = self.cal_total_fitness(self.pop)\n",
    "        self.population_size = population_size\n",
    "    def cal_total_fitness(self,popul):\n",
    "        total_fitness = 0.0\n",
    "        for individual in popul: \n",
    "            total_fitness += individual.fitness\n",
    "        return total_fitness\n",
    "    def create_population(self,population_size,dim,train_data,train_label,valid_data,valid_label):\n",
    "        pop = []\n",
    "        # no_individual is the number of individual\n",
    "        # dim is the dimension of individual\n",
    "        for i in range(population_size):\n",
    "            indi = individual(dim,train_data,train_label,valid_data,valid_label)\n",
    "            pop.append(indi)\n",
    "        return pop  \n",
    "    def sort_fitness(self):\n",
    "        self.pop.sort(key=attrgetter('fitness'),reverse=True)  \n",
    "def Roulette_Wheel_Selection(a_popu):\n",
    "    # Randomly select an individual to be a father/mother for next generation\n",
    "    # Chosing base on individual fitness, the one has higher fitness has highest probability to be chosen\n",
    "    total = 0.0\n",
    "    random_number = random.random()\n",
    "    #print random_number\n",
    "    for inv in a_popu.pop:\n",
    "        total += inv.probability\n",
    "        if random_number < total:\n",
    "            return inv\n",
    "def evaluation(a_population,train_data,train_label,valid_data,valid_label):\n",
    "    for indi in a_population.pop:\n",
    "        indi.fitness,indi.BER,indi.valid_accuracy = indi.fitness_evaluation(indi.ind ,\n",
    "                                                train_data,train_label,valid_data,valid_label, indi.dim)\n",
    "    a_population.total_fitness = a_population.cal_total_fitness(a_population.pop) \n",
    "    for indi in a_population.pop:\n",
    "        indi.update_probability(a_population.total_fitness)\n",
    "def selection(a_population):\n",
    "    sel = []\n",
    "    # Select M-1 individuals\n",
    "    for i in range(a_population.population_size-1):\n",
    "        sel.append(Roulette_Wheel_Selection(a_population))\n",
    "    for i in range(1,a_population.population_size):\n",
    "        a_population.pop[i] = sel[i-1]\n",
    "def cross_over(a_population,train_data,train_label,valid_data,valid_label,cross_over_probability = 1.0):\n",
    "    # Chosen a list of parents for next generation\n",
    "    # Shuffle the population, except the first one - the elite, which has the highest fitness\n",
    "    random.shuffle(a_population.pop[1:])\n",
    "    parent_pairs = zip(*(iter(a_population.pop),) * 2)\n",
    "    childs = []\n",
    "    for parent in parent_pairs: \n",
    "        # Create a random number from (0,1)\n",
    "        random_number = random.random()\n",
    "        # if this random number is smaller than cross_over_probability then we do cross over: \n",
    "        if random_number < cross_over_probability:\n",
    "            #print parent[0].dim\n",
    "            son = individual(parent[0].dim,train_data,train_label,valid_data,valid_label)\n",
    "            daughter = individual(parent[0].dim,train_data,train_label,valid_data,valid_label)\n",
    "            cross_over_point = random.randint(0,parent[0].dim)\n",
    "            for i in range(cross_over_point):\n",
    "                son.ind[i] = parent[0].ind[i]\n",
    "                daughter.ind[i] = parent[1].ind[i]\n",
    "            for i in range(cross_over_point,parent[0].dim):\n",
    "                son.ind[i] = parent[1].ind[i]\n",
    "                daughter.ind[i] = parent[0].ind[i]\n",
    "            # Check if the new child is all 0, then make it the same as parent\n",
    "            if not np.any(son):\n",
    "                son = parent[0]\n",
    "            if not np.any(daughter):\n",
    "                daughter = parent[1]\n",
    "        else:\n",
    "            son = parent[0]\n",
    "            daughter = parent[1]\n",
    "        childs.append(son)\n",
    "        childs.append(daughter)\n",
    "    for i in range(1,len(a_population.pop)):\n",
    "        a_population.pop[i] = childs[i-1]\n",
    "def mutation(a_population,mutation_probability = 0.001):\n",
    "    for indi in a_population.pop[1:]:\n",
    "        z = np.copy(indi)\n",
    "        # Create a random number from (0,1)\n",
    "        for i in range(len(indi.ind)):\n",
    "            random_number = random.random()\n",
    "            # if this random number is smaller than mutation_probability then we do mutation: \n",
    "            if random_number < mutation_probability:\n",
    "                indi.ind[i] = 1 - indi.ind[i]\n",
    "        # Check if the result of mutation is all 0, then make it the same as before\n",
    "        if not np.any(indi):\n",
    "            indi = z\n",
    "def GA(dim, num_iter, train_data, train_label,valid_data,valid_label):\n",
    "    \n",
    "    # Create population\n",
    "    population_size = 101\n",
    "    m = population(population_size,dim,train_data,train_label,valid_data,valid_label)\n",
    "    evaluation(m,train_data,train_label,valid_data,valid_label)\n",
    "    m.sort_fitness()\n",
    "    for count in range(num_iter):\n",
    "        selection(m)\n",
    "        cross_over(m,train_data,train_label,valid_data,valid_label)\n",
    "        mutation(m)\n",
    "        evaluation(m,train_data,train_label,valid_data,valid_label)\n",
    "        m.sort_fitness()\n",
    "    print('Best Result: BER, number of feature ')\n",
    "    print(m.pop[0].BER, sum(m.pop[0].ind))\n",
    "    return m.pop[0].BER, sum(m.pop[0].ind)\n",
    "\n",
    "def GA_multi(num,dim,train_data, train_label,valid_data,valid_label,name_dataset):\n",
    "    print('-----------------------------')\n",
    "    print('Dataset ', name_dataset)\n",
    "    full_feature(train_data,train_label,valid_data,valid_label)\n",
    "    for i in range(num):\n",
    "        print('Experiment', i + 1)\n",
    "        BER_array =[]\n",
    "        num_feature_array = []\n",
    "        BER, num_feature = GA(dim,train_data,train_label, valid_data,valid_label)\n",
    "        BER_array.append(BER)\n",
    "        num_feature_array.append(num_feature)\n",
    "        BER_mean = np.mean(BER_array)\n",
    "        num_feature_mean = np.mean(num_feature_array)\n",
    "        print('*******')\n",
    "    print('BER mean', BER_mean)\n",
    "    print('num feature mean', num_feature_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import timeit\n",
    "np.random.seed(920412)\n",
    "\n",
    "#This controls number of iterations\n",
    "epoch=10\n",
    "#This controls the amount of features from RF\n",
    "numoffeature=90\n",
    "\n",
    "\n",
    "def init(k):\n",
    "    h=[]\n",
    "    clf = RandomForestClassifier(n_estimators=500,\n",
    "                             criterion='gini', max_depth=10,  min_weight_fraction_leaf=0.0,\n",
    "                             max_features='auto' )\n",
    "    clf.fit(train_data, train_label)\n",
    "    importances = clf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    for f in range(k):\n",
    "        h.append(int(indices[f]))\n",
    "\n",
    "    return h\n",
    "\n",
    "def balanced_error_rate(y_true, y_test):\n",
    "    conf = confusion_matrix(y_true, y_test)\n",
    "    return 1-(0.5 * ((conf[0,1]*1.0) /conf[0].sum()) + 0.5 * ((1.0*conf[1,0]) / conf[1].sum()))\n",
    "\n",
    "def eval(h):\n",
    "    lr = LogisticRegression(C=1e-4,class_weight='balanced',max_iter=500)\n",
    "    lr.fit(train_data[:,h], train_label)\n",
    "    score = balanced_error_rate( valid_label ,lr.predict(valid_data[:,h]))\n",
    "    return score\n",
    "\n",
    "def update(p, epoch):\n",
    "    start=timeit.default_timer()\n",
    "    h=list()\n",
    "    addedlist=[]\n",
    "    oldlist=[]\n",
    "\n",
    "    oldscore=-1\n",
    "    highest=0.0\n",
    "    best=[]\n",
    "\n",
    "    k=100\n",
    "    while(k>=numoffeature): #0\n",
    "        for i in range(epoch): #1\n",
    "            start2=timeit.default_timer() #2\n",
    "            fvector=init(k) #2\n",
    "            end2=timeit.default_timer() #2\n",
    "\n",
    "            while len(fvector)<300: #2 \n",
    "                #print \"len(fvector): \",len(fvector)#3\n",
    "                if np.random.random()<p: #3\n",
    "\n",
    "                    key=np.random.multinomial(1, features / features.sum()).argmax() #4\n",
    "                    while(key in fvector): #4\n",
    "                        #print \"while multi\"\n",
    "                        key=np.random.multinomial(1, features / features.sum()).argmax() #5\n",
    "\n",
    "                else: #3\n",
    "\n",
    "                    key=np.random.randint(281,500,size=1)[0] #4\n",
    "                    while (key in fvector): #4\n",
    "                        key=np.random.randint(281,500,size=1)[0] #5\n",
    "\n",
    "                fvector.append(key) #3\n",
    "                addedlist.append(key) #3\n",
    "\n",
    "            score=eval(fvector)\n",
    "            if oldscore is not -1: #2\n",
    "                added=list(set(addedlist)-set(oldlist)) #3\n",
    "                removed=list(set(oldlist)-set(addedlist))#3\n",
    "                if score>oldscore: #3\n",
    "                    if score>highest: #4\n",
    "                        #print \">switch: \",\"score: \",score,\"highest: \",highest #5\n",
    "                        highest=score #5\n",
    "                        best=fvector #5\n",
    "                    for item in added: #4\n",
    "                        features[item]+=1 #5\n",
    "                    for item in removed: #4\n",
    "                        if features[item]>0: #5\n",
    "                            features[item]-=1 #6\n",
    "                else: #3\n",
    "                    if score>highest: #4\n",
    "                        highest=score #5\n",
    "                        best=fvector #5\n",
    "\n",
    "                    for item in removed: #4\n",
    "                        features[item]+=1 #5\n",
    "                    for item in added: #4\n",
    "                        if features[item]>0: #5\n",
    "                            features[item]-=1 #6\n",
    "            oldscore=score #2\n",
    "            print(\"k: \",k, \"Epoch: \",i, \"current: \", eval(fvector),\n",
    "              \"oldscore: \",oldscore,\"highest: \",highest,\"initialize time:\",end2-start2)\n",
    "        # Reduce amount of Feature from RF by 10\n",
    "        k-=10\n",
    "\n",
    "def loadARCENE():\n",
    "    training_file=\"ARCENE/arcene_train.data\"\n",
    "    training_label=\"ARCENE/arcene_train.labels\"\n",
    "    valid_file=\"ARCENE/arcene_valid.data\"\n",
    "    valid_label=\"ARCENE/arcene_valid.labels\"\n",
    "    train_data = np.loadtxt(training_file)\n",
    "    train_label = np.loadtxt(training_label)\n",
    "    valid_data = np.loadtxt(valid_file)\n",
    "    valid_label = np.loadtxt(valid_label)\n",
    "    return train_data,train_label,valid_data,valid_label\n",
    "\n",
    "def loadDOROTHEA():\n",
    "    train_data = np.load(\"DOROTHEA/train_array.npy\")\n",
    "    train_label = np.loadtxt(\"DOROTHEA/dorothea_train.labels\")\n",
    "    valid_data = np.load(\"DOROTHEA/test_array.npy\")\n",
    "    valid_label = np.loadtxt(\"DOROTHEA/dorothea_valid.labels\")\n",
    "    return train_data,train_label,valid_data,valid_label\n",
    "\n",
    "\n",
    "def loadDEXTER():\n",
    "    train_data = np.load(\"DEXTER/dexter_train.npy\")\n",
    "    train_label = np.loadtxt(\"DEXTER/dexter_train.labels\")\n",
    "    valid_data = np.load(\"DEXTER/dexter_valid.npy\")\n",
    "    valid_label = np.loadtxt(\"DEXTER/dexter_valid.labels\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCENE Data:\n",
      "Training data info: \n",
      "100  samples,  10000  features.\n",
      "44  positve samples,  56  negative samples\n",
      "-------------------------------\n",
      "Test data info: \n",
      "100  samples.\n",
      "44  positve samples,  56  negative samples\n",
      "Initial BER:  0.155844155844\n",
      "....................................................................................................0.121753246753 100\n",
      ".Features selected: 1739.0\n",
      "Final cost 0.124188311688\n",
      "Iterations run 101 \n",
      "Final cost  0.124188311688\n",
      "0.88\n",
      "(100, 8917)\n",
      "Initial Cost:  0.167207792208\n",
      "0.153409090909 1 \n",
      "\n",
      "4874.0\n",
      "0.153409090909\n",
      "0.126623376623 2 0.112824675325 3 ....0.11038961039 4 ......................0.107954545455 5 ...0.099025974026 6 .....................................................0.0876623376623 7 ...........Features selected: 745.0\n",
      "Final cost 0.0876623376623\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "Best Result: BER, number of feature \n",
      "0.0746753246753 2056\n",
      "Initializing the swarm....\n",
      "Iteration  1  Initial Best (BER, f1, num_feats, score): ( 0.128246753247 ,  0.857142857143 ,  1151 ,  0.129397753247 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.124188311688 ,  0.860465116279 ,  1708 ,  0.125896311688 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.124188311688 ,  0.860465116279 ,  1570 ,  0.125758311688 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.117694805195 ,  0.867469879518 ,  1225 ,  0.118919805195 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.112824675325 ,  0.873563218391 ,  818 ,  0.113642675325 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0949675324675 ,  0.894117647059 ,  1407 ,  0.0963745324675 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0949675324675 ,  0.894117647059 ,  1345 ,  0.0963125324675 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0746753246753 ,  0.917647058824 ,  1313 ,  0.0759883246753 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0746753246753 ,  0.917647058824 ,  1291 ,  0.0759663246753 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0746753246753 ,  0.917647058824 ,  1221 ,  0.0758963246753 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0746753246753 ,  0.917647058824 ,  1181 ,  0.0758563246753 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0746753246753 ,  0.917647058824 ,  1048 ,  0.0757233246753 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0722402597403 ,  0.919540229885 ,  1728 ,  0.0739682597403 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0657467532468 ,  0.928571428571 ,  1347 ,  0.0670937532468 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0657467532468 ,  0.928571428571 ,  1326 ,  0.0670727532468 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0657467532468 ,  0.928571428571 ,  1065 ,  0.0668117532468 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0543831168831 ,  0.941176470588 ,  1243 ,  0.0556261168831 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0543831168831 ,  0.941176470588 ,  1237 ,  0.0556201168831 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0543831168831 ,  0.941176470588 ,  1236 ,  0.0556191168831 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0543831168831 ,  0.941176470588 ,  1235 ,  0.0556181168831 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0543831168831 ,  0.941176470588 ,  1229 ,  0.0556121168831 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0543831168831 ,  0.941176470588 ,  1207 ,  0.0555901168831 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0543831168831 ,  0.941176470588 ,  1129 ,  0.0555121168831 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0543831168831 ,  0.941176470588 ,  1060 ,  0.0554431168831 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0543831168831 ,  0.941176470588 ,  1056 ,  0.0554391168831 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0543831168831 ,  0.941176470588 ,  887 ,  0.0552701168831 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0430194805195 ,  0.953488372093 ,  1048 ,  0.0440674805195 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0430194805195 ,  0.953488372093 ,  944 ,  0.0439634805195 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0430194805195 ,  0.953488372093 ,  904 ,  0.0439234805195 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0430194805195 ,  0.953488372093 ,  891 ,  0.0439104805195 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0430194805195 ,  0.953488372093 ,  883 ,  0.0439024805195 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0430194805195 ,  0.953488372093 ,  876 ,  0.0438954805195 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0430194805195 ,  0.953488372093 ,  763 ,  0.0437824805195 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0430194805195 ,  0.953488372093 ,  758 ,  0.0437774805195 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0430194805195 ,  0.953488372093 ,  756 ,  0.0437754805195 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0430194805195 ,  0.953488372093 ,  600 ,  0.0436194805195 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0405844155844 ,  0.954545454545 ,  637 ,  0.0412214155844 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0316558441558 ,  0.965517241379 ,  723 ,  0.0323788441558 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0316558441558 ,  0.965517241379 ,  638 ,  0.0322938441558 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0316558441558 ,  0.965517241379 ,  621 ,  0.0322768441558 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0316558441558 ,  0.965517241379 ,  609 ,  0.0322648441558 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0316558441558 ,  0.965517241379 ,  485 ,  0.0321408441558 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0202922077922 ,  0.977272727273 ,  492 ,  0.0207842077922 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0202922077922 ,  0.977272727273 ,  481 ,  0.0207732077922 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0202922077922 ,  0.977272727273 ,  479 ,  0.0207712077922 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0202922077922 ,  0.977272727273 ,  478 ,  0.0207702077922 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0202922077922 ,  0.977272727273 ,  474 ,  0.0207662077922 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0202922077922 ,  0.977272727273 ,  444 ,  0.0207362077922 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  443 ,  0.00937157142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  440 ,  0.00936857142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  438 ,  0.00936657142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  435 ,  0.00936357142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  433 ,  0.00936157142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  432 ,  0.00936057142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  405 ,  0.00933357142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  397 ,  0.00932557142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  395 ,  0.00932357142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  394 ,  0.00932257142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  392 ,  0.00932057142857 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  391 ,  0.00931957142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  386 ,  0.00931457142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  385 ,  0.00931357142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  369 ,  0.00929757142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  352 ,  0.00928057142857 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.00892857142857 ,  0.988764044944 ,  351 ,  0.00927957142857 )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time test on Arcene\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data('arcene')\n",
    "clf = LogisticRegression(C=0.0001, class_weight='balanced')\n",
    "\n",
    "\n",
    "# SA\n",
    "start = dft()\n",
    "run_SA(X_train, y_train, X_test, y_test, max_iter=100, nbr=40, bits=4, nbr_code=3, temp_code=2)\n",
    "sa_time_a = dft() - start\n",
    "\n",
    "# HC \n",
    "start = dft()\n",
    "run_HC(X_train, y_train, X_test, y_test, max_iter=100, restart_iter=200, nbr=40, bits=40, nbr_code=3)\n",
    "hc_time_a = dft() - start\n",
    "\n",
    "# GA\n",
    "start = dft()\n",
    "GA(X_train.shape[1], 100, X_train, y_train, X_test, y_test)\n",
    "ga_time_a = dft() - start\n",
    "\n",
    "# PSO\n",
    "start = dft()\n",
    "run_pso(150, 100, 0.01, clf)\n",
    "pso_time_a = dft() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARCENE Data:\n",
      "Training data info: \n",
      "100  samples,  10000  features.\n",
      "44  positve samples,  56  negative samples\n",
      "-------------------------------\n",
      "Test data info: \n",
      "100  samples.\n",
      "44  positve samples,  56  negative samples\n",
      "k:  100 Epoch:  0 current:  0.785714285714 oldscore:  0.785714285714 highest:  0.0 initialize time: 0.8655324569990626\n",
      "k:  100 Epoch:  1 current:  0.747564935065 oldscore:  0.747564935065 highest:  0.747564935065 initialize time: 0.8694560189978802\n",
      "k:  100 Epoch:  2 current:  0.788149350649 oldscore:  0.788149350649 highest:  0.788149350649 initialize time: 0.8593421149998903\n",
      "k:  100 Epoch:  3 current:  0.785714285714 oldscore:  0.785714285714 highest:  0.788149350649 initialize time: 0.8541306990009616\n",
      "k:  100 Epoch:  4 current:  0.747564935065 oldscore:  0.747564935065 highest:  0.788149350649 initialize time: 0.8476683919943753\n",
      "k:  100 Epoch:  5 current:  0.806006493506 oldscore:  0.806006493506 highest:  0.806006493506 initialize time: 0.8565831980013172\n",
      "k:  100 Epoch:  6 current:  0.788149350649 oldscore:  0.788149350649 highest:  0.806006493506 initialize time: 0.8593527769990033\n",
      "k:  100 Epoch:  7 current:  0.774350649351 oldscore:  0.774350649351 highest:  0.806006493506 initialize time: 0.867281967002782\n",
      "k:  100 Epoch:  8 current:  0.788149350649 oldscore:  0.788149350649 highest:  0.806006493506 initialize time: 0.8909278919963981\n",
      "k:  100 Epoch:  9 current:  0.799512987013 oldscore:  0.799512987013 highest:  0.806006493506 initialize time: 0.8964974089976749\n",
      "k:  100 Epoch:  10 current:  0.792207792208 oldscore:  0.792207792208 highest:  0.806006493506 initialize time: 0.8921401429979596\n",
      "k:  100 Epoch:  11 current:  0.776785714286 oldscore:  0.776785714286 highest:  0.806006493506 initialize time: 0.8901013149952632\n",
      "k:  100 Epoch:  12 current:  0.774350649351 oldscore:  0.774350649351 highest:  0.806006493506 initialize time: 0.8957175230025314\n",
      "k:  100 Epoch:  13 current:  0.774350649351 oldscore:  0.774350649351 highest:  0.806006493506 initialize time: 0.9016307040001266\n",
      "k:  100 Epoch:  14 current:  0.751623376623 oldscore:  0.751623376623 highest:  0.806006493506 initialize time: 0.8923390170020866\n",
      "k:  100 Epoch:  15 current:  0.828733766234 oldscore:  0.828733766234 highest:  0.828733766234 initialize time: 0.9060231650000787\n",
      "k:  100 Epoch:  16 current:  0.756493506494 oldscore:  0.756493506494 highest:  0.828733766234 initialize time: 0.8931805770043866\n",
      "k:  100 Epoch:  17 current:  0.823863636364 oldscore:  0.823863636364 highest:  0.828733766234 initialize time: 0.9047254179968149\n",
      "k:  100 Epoch:  18 current:  0.754058441558 oldscore:  0.754058441558 highest:  0.828733766234 initialize time: 0.9027997799930745\n",
      "k:  100 Epoch:  19 current:  0.776785714286 oldscore:  0.776785714286 highest:  0.828733766234 initialize time: 0.9017069620022085\n",
      "k:  100 Epoch:  20 current:  0.771915584416 oldscore:  0.771915584416 highest:  0.828733766234 initialize time: 0.8944932710001012\n",
      "k:  100 Epoch:  21 current:  0.756493506494 oldscore:  0.756493506494 highest:  0.828733766234 initialize time: 0.8958509340009186\n",
      "k:  100 Epoch:  22 current:  0.810876623377 oldscore:  0.810876623377 highest:  0.828733766234 initialize time: 0.8893399169974145\n",
      "k:  100 Epoch:  23 current:  0.770292207792 oldscore:  0.770292207792 highest:  0.828733766234 initialize time: 0.9037697750027291\n",
      "k:  100 Epoch:  24 current:  0.74025974026 oldscore:  0.74025974026 highest:  0.828733766234 initialize time: 0.9030421019997448\n",
      "k:  100 Epoch:  25 current:  0.808441558442 oldscore:  0.808441558442 highest:  0.828733766234 initialize time: 0.8948980940040201\n",
      "k:  100 Epoch:  26 current:  0.776785714286 oldscore:  0.776785714286 highest:  0.828733766234 initialize time: 0.8690355700018699\n",
      "k:  100 Epoch:  27 current:  0.806006493506 oldscore:  0.806006493506 highest:  0.828733766234 initialize time: 0.8870854660053737\n",
      "k:  100 Epoch:  28 current:  0.767857142857 oldscore:  0.767857142857 highest:  0.828733766234 initialize time: 0.875822424000944\n",
      "k:  100 Epoch:  29 current:  0.765422077922 oldscore:  0.765422077922 highest:  0.828733766234 initialize time: 0.8896406979984022\n",
      "k:  100 Epoch:  30 current:  0.806006493506 oldscore:  0.806006493506 highest:  0.828733766234 initialize time: 0.8746324429957895\n",
      "k:  100 Epoch:  31 current:  0.761363636364 oldscore:  0.761363636364 highest:  0.828733766234 initialize time: 0.8838058350011124\n",
      "k:  100 Epoch:  32 current:  0.797077922078 oldscore:  0.797077922078 highest:  0.828733766234 initialize time: 0.8864964870008407\n",
      "k:  100 Epoch:  33 current:  0.762987012987 oldscore:  0.762987012987 highest:  0.828733766234 initialize time: 0.8838827260042308\n",
      "k:  100 Epoch:  34 current:  0.747564935065 oldscore:  0.747564935065 highest:  0.828733766234 initialize time: 0.8878533120005159\n",
      "k:  100 Epoch:  35 current:  0.776785714286 oldscore:  0.776785714286 highest:  0.828733766234 initialize time: 0.88176383300015\n",
      "k:  100 Epoch:  36 current:  0.779220779221 oldscore:  0.779220779221 highest:  0.828733766234 initialize time: 0.8821898050009622\n",
      "k:  100 Epoch:  37 current:  0.762987012987 oldscore:  0.762987012987 highest:  0.828733766234 initialize time: 0.9109531450012582\n",
      "k:  100 Epoch:  38 current:  0.81737012987 oldscore:  0.81737012987 highest:  0.828733766234 initialize time: 0.9225720680042286\n",
      "k:  100 Epoch:  39 current:  0.806006493506 oldscore:  0.806006493506 highest:  0.828733766234 initialize time: 0.9064084269994055\n",
      "k:  100 Epoch:  40 current:  0.806006493506 oldscore:  0.806006493506 highest:  0.828733766234 initialize time: 0.9471506350018899\n",
      "k:  100 Epoch:  41 current:  0.828733766234 oldscore:  0.828733766234 highest:  0.828733766234 initialize time: 0.9252540939996834\n",
      "k:  100 Epoch:  42 current:  0.788149350649 oldscore:  0.788149350649 highest:  0.828733766234 initialize time: 0.8850969390041428\n",
      "k:  100 Epoch:  43 current:  0.819805194805 oldscore:  0.819805194805 highest:  0.828733766234 initialize time: 0.9051327650013263\n",
      "k:  100 Epoch:  44 current:  0.801948051948 oldscore:  0.801948051948 highest:  0.828733766234 initialize time: 0.9093291079989285\n",
      "k:  100 Epoch:  45 current:  0.776785714286 oldscore:  0.776785714286 highest:  0.828733766234 initialize time: 0.9037381870002719\n",
      "k:  100 Epoch:  46 current:  0.788149350649 oldscore:  0.788149350649 highest:  0.828733766234 initialize time: 0.9255295240000123\n",
      "k:  100 Epoch:  47 current:  0.741071428571 oldscore:  0.741071428571 highest:  0.828733766234 initialize time: 0.8921604439965449\n",
      "k:  100 Epoch:  48 current:  0.797077922078 oldscore:  0.797077922078 highest:  0.828733766234 initialize time: 0.9075570880013402\n",
      "k:  100 Epoch:  49 current:  0.729707792208 oldscore:  0.729707792208 highest:  0.828733766234 initialize time: 0.8912914270040346\n",
      "k:  100 Epoch:  50 current:  0.776785714286 oldscore:  0.776785714286 highest:  0.828733766234 initialize time: 0.8620157500045025\n",
      "k:  100 Epoch:  51 current:  0.747564935065 oldscore:  0.747564935065 highest:  0.828733766234 initialize time: 0.8642556569975568\n",
      "k:  100 Epoch:  52 current:  0.819805194805 oldscore:  0.819805194805 highest:  0.828733766234 initialize time: 0.8621451340004569\n",
      "k:  100 Epoch:  53 current:  0.747564935065 oldscore:  0.747564935065 highest:  0.828733766234 initialize time: 0.8580975619988749\n",
      "k:  100 Epoch:  54 current:  0.788149350649 oldscore:  0.788149350649 highest:  0.828733766234 initialize time: 0.8600306400039699\n",
      "k:  100 Epoch:  55 current:  0.785714285714 oldscore:  0.785714285714 highest:  0.828733766234 initialize time: 0.8627229659978184\n",
      "k:  100 Epoch:  56 current:  0.797077922078 oldscore:  0.797077922078 highest:  0.828733766234 initialize time: 0.8525894720005454\n",
      "k:  100 Epoch:  57 current:  0.756493506494 oldscore:  0.756493506494 highest:  0.828733766234 initialize time: 0.857975085004\n",
      "k:  100 Epoch:  58 current:  0.8125 oldscore:  0.8125 highest:  0.828733766234 initialize time: 0.8523294149999856\n",
      "k:  100 Epoch:  59 current:  0.754058441558 oldscore:  0.754058441558 highest:  0.828733766234 initialize time: 0.8620492629997898\n",
      "k:  100 Epoch:  60 current:  0.788149350649 oldscore:  0.788149350649 highest:  0.828733766234 initialize time: 0.8523556779982755\n",
      "k:  100 Epoch:  61 current:  0.81737012987 oldscore:  0.81737012987 highest:  0.828733766234 initialize time: 0.8594550440029707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:  100 Epoch:  62 current:  0.774350649351 oldscore:  0.774350649351 highest:  0.828733766234 initialize time: 0.8529958759972942\n",
      "k:  100 Epoch:  63 current:  0.803571428571 oldscore:  0.803571428571 highest:  0.828733766234 initialize time: 0.8575503269967157\n",
      "k:  100 Epoch:  64 current:  0.788149350649 oldscore:  0.788149350649 highest:  0.828733766234 initialize time: 0.8574928599991836\n",
      "k:  100 Epoch:  65 current:  0.789772727273 oldscore:  0.789772727273 highest:  0.828733766234 initialize time: 0.861298331998114\n",
      "k:  100 Epoch:  66 current:  0.788149350649 oldscore:  0.788149350649 highest:  0.828733766234 initialize time: 0.8578569259989308\n",
      "k:  100 Epoch:  67 current:  0.806006493506 oldscore:  0.806006493506 highest:  0.828733766234 initialize time: 0.8774801850013318\n",
      "k:  100 Epoch:  68 current:  0.770292207792 oldscore:  0.770292207792 highest:  0.828733766234 initialize time: 0.8591143510057009\n",
      "k:  100 Epoch:  69 current:  0.806006493506 oldscore:  0.806006493506 highest:  0.828733766234 initialize time: 0.8588400430016918\n",
      "k:  100 Epoch:  70 current:  0.774350649351 oldscore:  0.774350649351 highest:  0.828733766234 initialize time: 0.9809059709950816\n",
      "k:  100 Epoch:  71 current:  0.74512987013 oldscore:  0.74512987013 highest:  0.828733766234 initialize time: 0.8656227540050168\n",
      "k:  100 Epoch:  72 current:  0.801136363636 oldscore:  0.801136363636 highest:  0.828733766234 initialize time: 0.8594992270009243\n",
      "k:  100 Epoch:  73 current:  0.767857142857 oldscore:  0.767857142857 highest:  0.828733766234 initialize time: 0.8631083040017984\n",
      "k:  100 Epoch:  74 current:  0.779220779221 oldscore:  0.779220779221 highest:  0.828733766234 initialize time: 0.8580681219973485\n",
      "k:  100 Epoch:  75 current:  0.832792207792 oldscore:  0.832792207792 highest:  0.832792207792 initialize time: 0.8552679979984532\n",
      "k:  100 Epoch:  76 current:  0.758928571429 oldscore:  0.758928571429 highest:  0.832792207792 initialize time: 0.855847115002689\n",
      "k:  100 Epoch:  77 current:  0.733766233766 oldscore:  0.733766233766 highest:  0.832792207792 initialize time: 0.8603905759955524\n",
      "k:  100 Epoch:  78 current:  0.799512987013 oldscore:  0.799512987013 highest:  0.832792207792 initialize time: 0.857142754997767\n",
      "k:  100 Epoch:  79 current:  0.779220779221 oldscore:  0.779220779221 highest:  0.832792207792 initialize time: 0.8663312509961543\n",
      "k:  100 Epoch:  80 current:  0.779220779221 oldscore:  0.779220779221 highest:  0.832792207792 initialize time: 0.8524000620018342\n",
      "k:  100 Epoch:  81 current:  0.794642857143 oldscore:  0.794642857143 highest:  0.832792207792 initialize time: 0.8648858609958552\n",
      "k:  100 Epoch:  82 current:  0.767857142857 oldscore:  0.767857142857 highest:  0.832792207792 initialize time: 0.886075415997766\n",
      "k:  100 Epoch:  83 current:  0.792207792208 oldscore:  0.792207792208 highest:  0.832792207792 initialize time: 0.9081127960016602\n",
      "k:  100 Epoch:  84 current:  0.823863636364 oldscore:  0.823863636364 highest:  0.832792207792 initialize time: 0.8795943470031489\n",
      "k:  100 Epoch:  85 current:  0.828733766234 oldscore:  0.828733766234 highest:  0.832792207792 initialize time: 0.8960343190046842\n",
      "k:  100 Epoch:  86 current:  0.814935064935 oldscore:  0.814935064935 highest:  0.832792207792 initialize time: 0.8905733930005226\n",
      "k:  100 Epoch:  87 current:  0.806006493506 oldscore:  0.806006493506 highest:  0.832792207792 initialize time: 0.8781507370003965\n",
      "k:  100 Epoch:  88 current:  0.772727272727 oldscore:  0.772727272727 highest:  0.832792207792 initialize time: 0.8752562530062278\n",
      "k:  100 Epoch:  89 current:  0.749188311688 oldscore:  0.749188311688 highest:  0.832792207792 initialize time: 0.8925887939985842\n",
      "k:  100 Epoch:  90 current:  0.761363636364 oldscore:  0.761363636364 highest:  0.832792207792 initialize time: 0.8627787669975078\n",
      "k:  100 Epoch:  91 current:  0.783279220779 oldscore:  0.783279220779 highest:  0.832792207792 initialize time: 0.8541717350017279\n",
      "k:  100 Epoch:  92 current:  0.776785714286 oldscore:  0.776785714286 highest:  0.832792207792 initialize time: 0.8562521499989089\n",
      "k:  100 Epoch:  93 current:  0.776785714286 oldscore:  0.776785714286 highest:  0.832792207792 initialize time: 0.8966203500021948\n",
      "k:  100 Epoch:  94 current:  0.814935064935 oldscore:  0.814935064935 highest:  0.832792207792 initialize time: 0.9059865240051295\n",
      "k:  100 Epoch:  95 current:  0.785714285714 oldscore:  0.785714285714 highest:  0.832792207792 initialize time: 0.9257174140002462\n",
      "k:  100 Epoch:  96 current:  0.814935064935 oldscore:  0.814935064935 highest:  0.832792207792 initialize time: 0.87100639900018\n",
      "k:  100 Epoch:  97 current:  0.792207792208 oldscore:  0.792207792208 highest:  0.832792207792 initialize time: 0.8817559989984147\n",
      "k:  100 Epoch:  98 current:  0.706980519481 oldscore:  0.706980519481 highest:  0.832792207792 initialize time: 0.9054819209995912\n",
      "k:  100 Epoch:  99 current:  0.783279220779 oldscore:  0.783279220779 highest:  0.832792207792 initialize time: 0.9479793490027077\n",
      "k:  90 Epoch:  0 current:  0.751623376623 oldscore:  0.751623376623 highest:  0.832792207792 initialize time: 0.8877408490006928\n",
      "k:  90 Epoch:  1 current:  0.74512987013 oldscore:  0.74512987013 highest:  0.832792207792 initialize time: 0.8777206599988858\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-e59d7cd47460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtb_time_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-af6b82909278>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(p, epoch)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                         \u001b[0;31m#print \"while multi\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data,train_label,valid_data,valid_label=load_data('arcene')\n",
    "start = dft()\n",
    "features=np.ones(train_data.shape[1])\n",
    "update(0.85, 100)\n",
    "tb_time_a = dft() - start()\n",
    "\n",
    "train_data,train_label,valid_data,valid_label=load_data('dexter')\n",
    "start = dft()\n",
    "features=np.ones(train_data.shape[1])\n",
    "update(0.85, 100)\n",
    "tb_time_dx = dft() - start()\n",
    "\n",
    "train_data,train_label,valid_data,valid_label=load_data('dorothea')\n",
    "start = dft()\n",
    "features=np.ones(train_data.shape[1])\n",
    "update(0.85, 100)\n",
    "tb_time_d = dft() - start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f403072e9b0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFnCAYAAACLnxFFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4034a441d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "times = [hc_time_a / 100, sa_time_a/100, ga_time_a/100, pso_time_a/100]\n",
    "\n",
    "g = sns.barplot(x=['HC', 'SA', 'GA', 'PSO'],y=times,\n",
    "                palette=sns.cubehelix_palette(8))\n",
    "plt.title('Average time per iteration after 100 iterations: Arcene')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Average time (seconds / iteration)')\n",
    "plt.ylim((0, 10))\n",
    "g.text(-0.09, 2.1, str(round(times[0],2)))\n",
    "g.text(1-0.09, 2.1, str(round(times[1],2)))\n",
    "g.text(2-0.09, 8.7, str(round(times[2],2)))\n",
    "g.text(3-0.09, 5, str(round(times[3],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEXTER Data:\n",
      "Training data info: \n",
      "300  samples,  20000  features.\n",
      "150  positve samples,  150  negative samples\n",
      "-------------------------------\n",
      "Test data info: \n",
      "300  samples.\n",
      "150  positve samples,  150  negative samples\n",
      "Initial BER:  0.136666666667\n",
      "....................................................................................................0.05 100\n",
      ".Features selected: 15037.0\n",
      "Final cost 0.0433333333333\n",
      "Iterations run 101 \n",
      "Final cost  0.0433333333333\n",
      "0.956666666667\n",
      "(300, 2545)\n",
      "Initial Cost:  0.27\n",
      "0.0633333333333 1 \n",
      "\n",
      "9060.0\n",
      "0.0633333333333\n",
      "0.06 2 .0.0533333333333 3 0.05 4 .0.0466666666667 5 .......0.0433333333333 6 0.0366666666667 7 ...................................................................0.0333333333333 8 ................Features selected: 10841.0\n",
      "Final cost 0.0333333333333\n",
      "[ 0.  1.  1.  0.  1.  0.  0.  1.  1.  0.  1.  0.  1.  0.  0.  1.  1.  0.\n",
      "  1.  1.  1.  0.  0.  0.  1.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  1.\n",
      "  0.  1.  1.  0.  1.  0.  1.  0.  1.  1.  0.  0.  1.  1.]\n",
      "Best Result: BER, number of feature \n",
      "0.0233333333333 12455\n",
      "Initializing the swarm....\n",
      "Iteration  1  Initial Best (BER, f1, num_feats, score): ( 0.0566666666667 ,  0.942760942761 ,  9338 ,  0.0613356666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.05 ,  0.951140065147 ,  11523 ,  0.0557615 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.04 ,  0.960264900662 ,  12263 ,  0.0461315 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0366666666667 ,  0.963696369637 ,  12046 ,  0.0426896666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0366666666667 ,  0.963210702341 ,  10508 ,  0.0419206666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.03 ,  0.970099667774 ,  13456 ,  0.036728 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.03 ,  0.970099667774 ,  13211 ,  0.0366055 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.03 ,  0.970099667774 ,  13023 ,  0.0365115 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.03 ,  0.970099667774 ,  12291 ,  0.0361455 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0266666666667 ,  0.973333333333 ,  12082 ,  0.0327076666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0266666666667 ,  0.973333333333 ,  11901 ,  0.0326171666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0266666666667 ,  0.973333333333 ,  11854 ,  0.0325936666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0266666666667 ,  0.973154362416 ,  10773 ,  0.0320531666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0233333333333 ,  0.976897689769 ,  11606 ,  0.0291363333333 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0233333333333 ,  0.976588628763 ,  11426 ,  0.0290463333333 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0233333333333 ,  0.976588628763 ,  11149 ,  0.0289078333333 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0233333333333 ,  0.976588628763 ,  11083 ,  0.0288748333333 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  12050 ,  0.026025 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11994 ,  0.025997 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11937 ,  0.0259685 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11875 ,  0.0259375 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11647 ,  0.0258235 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11610 ,  0.025805 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11561 ,  0.0257805 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11547 ,  0.0257735 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11496 ,  0.025748 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11475 ,  0.0257375 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11393 ,  0.0256965 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11317 ,  0.0256585 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11269 ,  0.0256345 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11266 ,  0.025633 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11216 ,  0.025608 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  11070 ,  0.025535 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  10993 ,  0.0254965 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  10974 ,  0.025487 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  10938 ,  0.025469 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  10935 ,  0.0254675 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.02 ,  0.98 ,  10932 ,  0.025466 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  11065 ,  0.0221991666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  11043 ,  0.0221881666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  11002 ,  0.0221676666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10975 ,  0.0221541666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10970 ,  0.0221516666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10965 ,  0.0221491666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10958 ,  0.0221456666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10945 ,  0.0221391666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10932 ,  0.0221326666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10891 ,  0.0221121666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10883 ,  0.0221081666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10882 ,  0.0221076666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10755 ,  0.0220441666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10748 ,  0.0220406666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10690 ,  0.0220116666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10683 ,  0.0220081666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10668 ,  0.0220006666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10663 ,  0.0219981666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10657 ,  0.0219951666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10647 ,  0.0219901666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10646 ,  0.0219896666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10634 ,  0.0219836666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10631 ,  0.0219821666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10623 ,  0.0219781666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10611 ,  0.0219721666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10569 ,  0.0219511666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10561 ,  0.0219471666667 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10558 ,  0.0219456666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10548 ,  0.0219406666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10543 ,  0.0219381666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10542 ,  0.0219376666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10430 ,  0.0218816666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10077 ,  0.0217051666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  10030 ,  0.0216816666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  9996 ,  0.0216646666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  9988 ,  0.0216606666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  9970 ,  0.0216516666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  9804 ,  0.0215686666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  9738 ,  0.0215356666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  9654 ,  0.0214936666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  9634 ,  0.0214836666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  9620 ,  0.0214766666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  9585 ,  0.0214591666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  9517 ,  0.0214251666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  9492 ,  0.0214126666667 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0166666666667 ,  0.983388704319 ,  9462 ,  0.0213976666667 )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time test on Dexter\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data('dexter')\n",
    "clf = LogisticRegression(C=0.0001, class_weight='balanced')\n",
    "\n",
    "\n",
    "# SA\n",
    "start = dft()\n",
    "run_SA(X_train, y_train, X_test, y_test, max_iter=100, nbr=40, bits=4, nbr_code=3, temp_code=2)\n",
    "sa_time_dx = dft() - start\n",
    "\n",
    "# HC \n",
    "start = dft()\n",
    "run_HC(X_train, y_train, X_test, y_test, max_iter=100, restart_iter=200, nbr=40, bits=40, nbr_code=3)\n",
    "hc_time_dx = dft() - start\n",
    "\n",
    "# GA\n",
    "start = dft()\n",
    "GA(X_train.shape[1], 100, X_train, y_train, X_test, y_test)\n",
    "ga_time_dx = dft() - start\n",
    "\n",
    "# PSO\n",
    "start = dft()\n",
    "run_pso(150, 100, 0.01, clf)\n",
    "pso_time_dx = dft() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f402be9d390>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFnCAYAAACLnxFFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4020d13f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "times = [hc_time_dx / 100, sa_time_dx/100, ga_time_dx/100, pso_time_dx/100]\n",
    "\n",
    "g = sns.barplot(x=['HC', 'SA', 'GA', 'PSO'],y=times,\n",
    "                palette=sns.cubehelix_palette(8))\n",
    "plt.title('Average time per iteration after 100 iterations: Dexter')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Average time (seconds / iteration)')\n",
    "plt.ylim((0, 14))\n",
    "g.text(-0.09, 2.1, str(round(times[0],2)))\n",
    "g.text(1-0.09, 2.1, str(round(times[1],2)))\n",
    "g.text(2-0.09, 10.1, str(round(times[2],2)))\n",
    "g.text(3-0.09, 12.5, str(round(times[3],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOROTHEA Data:\n",
      "Training data info: \n",
      "800  samples,  100000  features.\n",
      "78  positve samples,  722  negative samples\n",
      "-------------------------------\n",
      "Test data info: \n",
      "350  samples.\n",
      "34  positve samples,  316  negative samples\n",
      "Initial BER:  0.119880863738\n",
      "...........Features selected: 54740.0\n",
      "Final cost 0.08730454207\n",
      "Iterations run 11 \n",
      "Final cost  0.08730454207\n",
      "0.937142857143\n",
      "(800, 4563)\n",
      "Initial Cost:  0.236504095309\n",
      "0.111038719285 1 \n",
      "\n",
      "37694.0\n",
      "0.111038719285\n",
      "0.0952159344751 2 0.08730454207 3 .......Features selected: 56939.0\n",
      "Final cost 0.08730454207\n",
      "[ 1.  1.  0.  1.  0.  1.  1.  0.  0.  1.  1.  1.  1.  1.  0.  1.  0.  0.\n",
      "  1.  1.  0.  1.  0.  1.  1.  1.  1.  0.  1.  0.  1.  0.  0.  1.  1.  1.\n",
      "  1.  0.  0.  0.  0.  1.  0.  1.  1.  1.  1.  0.  0.  0.]\n",
      "Best Result: BER, number of feature \n",
      "0.0705510052122 36506\n",
      "Initializing the swarm....\n",
      "Iteration  1  Initial Best (BER, f1, num_feats, score): ( 0.104709605361 ,  0.645161290323 ,  40114 ,  0.108721005361 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0947505584512 ,  0.639175257732 ,  38017 ,  0.0985522584512 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0947505584512 ,  0.639175257732 ,  36387 ,  0.0983892584512 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0847915115413 ,  0.633663366337 ,  26824 ,  0.0874739115413 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0820923306031 ,  0.696629213483 ,  50302 ,  0.0871225306031 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0705510052122 ,  0.695652173913 ,  44879 ,  0.0750389052122 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0689687267312 ,  0.703296703297 ,  40868 ,  0.0730555267312 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0689687267312 ,  0.703296703297 ,  37636 ,  0.0727323267312 )\n",
      " Iteration  1  Current Best (BER, f1, num_feats, score): ( 0.0626396128071 ,  0.735632183908 ,  39978 ,  0.0666374128071 )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time test on Dorothea\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data('dorothea')\n",
    "clf = LogisticRegression(C=0.0001, class_weight='balanced')\n",
    "\n",
    "\n",
    "# SA\n",
    "start = dft()\n",
    "run_SA(X_train, y_train, X_test, y_test, max_iter=10, nbr=40, bits=4, nbr_code=3, temp_code=2)\n",
    "sa_time_d = dft() - start\n",
    "\n",
    "# HC \n",
    "start = dft()\n",
    "run_HC(X_train, y_train, X_test, y_test, max_iter=10, restart_iter=200, nbr=40, bits=40, nbr_code=3)\n",
    "hc_time_d = dft() - start\n",
    "\n",
    "# GA\n",
    "start = dft()\n",
    "GA(X_train.shape[1], 10, X_train, y_train, X_test, y_test)\n",
    "ga_time_d = dft() - start\n",
    "\n",
    "# PSO\n",
    "start = dft()\n",
    "run_pso(150, 10, 0.01, clf)\n",
    "pso_time_d = dft() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f402bbd1828>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFnCAYAAABdOssgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIhamIBcREbEwBbmIiIiFKchFREQs7DvQsrGEYVLtIwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4030030940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "times = [hc_time_d / 10, sa_time_d/10, ga_time_d/10, pso_time_d/10]\n",
    "\n",
    "g = sns.barplot(x=['HC', 'SA', 'GA', 'PSO'],y=times,\n",
    "                palette=sns.cubehelix_palette(8))\n",
    "plt.title('Average time per iteration after 10 iterations: Dorothea')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Average time (seconds / iteration)')\n",
    "plt.ylim((0, 150))\n",
    "g.text(-0.09, 25, str(round(times[0],2)))\n",
    "g.text(1-0.09, 30, str(round(times[1],2)))\n",
    "g.text(2-0.09, 142, str(round(times[2],2)))\n",
    "g.text(3-0.09, 130, str(round(times[3],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "45b7f1c4c96440f2b684e716ad102a89": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "9a5c7183c08947b8a1947e7e24d6d127": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "bd1859eea7844b18a4462470e3135d56": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
